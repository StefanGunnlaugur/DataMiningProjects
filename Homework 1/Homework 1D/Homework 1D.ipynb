{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Axis-Parallel Subspace Clustering (20 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "import pandas as pd \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = 3\n",
    "data = pd.read_csv(\"iris_csv.csv\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def most_frequent(List): \n",
    "    b = Counter(List)\n",
    "    return b.most_common(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## α. (6 points) \n",
    "\n",
    "Use the k-means algorithm with k = 3 to cluster the Iris dataset, over the whole 4-\n",
    "dimensional input space. How well do the clusters match the actual labels?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is: 89.33333333333333 %\n"
     ]
    }
   ],
   "source": [
    "a_data = data.copy()\n",
    "\n",
    "classes = a_data['class'].copy()\n",
    "\n",
    "X_train = a_data.drop('class', axis=1)\n",
    "        \n",
    "\n",
    "kmeans = KMeans(init='k-means++', n_clusters=n_classes, n_init=10)\n",
    "kmeans.fit(X_train)\n",
    "\n",
    "y_pred = kmeans.fit_predict(X_train)\n",
    "\n",
    "label1 = most_frequent(y_pred[:50])\n",
    "label2 = most_frequent(y_pred[50:100])\n",
    "label3 = most_frequent(y_pred[100:150])\n",
    "\n",
    "for i in range(classes.size):\n",
    "    if classes[i] == \"Iris-setosa\":\n",
    "        classes[i] = label1[0][0]\n",
    "    if classes[i] == \"Iris-versicolor\":\n",
    "        classes[i] = label2[0][0]\n",
    "    if classes[i] == \"Iris-virginica\":\n",
    "        classes[i] = label3[0][0]\n",
    "        \n",
    "errors = 0\n",
    "for i in range(y_pred.size):\n",
    "    if y_pred[i] != classes[i]:\n",
    "        errors = errors + 1\n",
    "\n",
    "print('Accuracy is:', ((y_pred.size - errors)/y_pred.size)*100, '%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The clusters are clustered around 89% correct. Because K-Means is not a classifier but a cluster algorithm, the cluster labels that k-means returns are in a quite random order so we check witch label is most common for each part of the dataset, representing each class of the flower. We then use the most common label as the correct label for the type of flower. In this exersice we can trust this gives the right accuarcy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## β. (4 points) \n",
    "\n",
    "Project the dataset axis-parallel onto the dimensions Sepal.Length and Sepal.Width\n",
    "(hint: probably the easiest way of doing this is by simply throwing away the other columns). Then,\n",
    "run the k-means algorithm with k = 3 again on the projected dataset. Do the results improve?\n",
    "Do they deteriorate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is: 82.0 %\n"
     ]
    }
   ],
   "source": [
    "b_data = data.copy()\n",
    "B_train = b_data.drop(['petallength','petalwidth','class'], axis=1)\n",
    "\n",
    "classes = b_data['class'].copy()\n",
    "\n",
    "b_n_classes = 3\n",
    "\n",
    "kmeans = KMeans(n_clusters=b_n_classes, n_init=10)\n",
    "kmeans.fit(B_train)\n",
    "\n",
    "y_pred = kmeans.fit_predict(B_train)\n",
    "\n",
    "label1 = most_frequent(y_pred[:50])\n",
    "label2 = most_frequent(y_pred[50:100])\n",
    "label3 = most_frequent(y_pred[100:150])\n",
    "\n",
    "for i in range(classes.size):\n",
    "    if classes[i] == \"Iris-setosa\":\n",
    "        classes[i] = label1[0][0]\n",
    "    if classes[i] == \"Iris-versicolor\":\n",
    "        classes[i] = label2[0][0]\n",
    "    if classes[i] == \"Iris-virginica\":\n",
    "        classes[i] = label3[0][0]\n",
    "        \n",
    "errors = 0\n",
    "for i in range(y_pred.size):\n",
    "    if y_pred[i] != classes[i]:\n",
    "        errors = errors + 1\n",
    "\n",
    "print('Accuracy is:', ((y_pred.size - errors)/y_pred.size)*100, '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we get an accuracy of 82% which is worse than before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## γ. (10 points) \n",
    "\n",
    "Repeat the previous exercise with each possible two-dimensional axis-parallel subspace\n",
    "(i.e.: five more times). In which subspace does the clustering mimic the true labels of the dataset\n",
    "most closely? Which type of Iris most often ends up in a wrong cluster? Which records are\n",
    "particularly difficult to cluster?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for dropped sepallength and sepalwidth is: 96.0 %\n",
      "Errors for Iris-setosa is: 0\n",
      "Errors for Iris-virginica is: 2\n",
      "Errors for Iris-versicolor is: 4\n",
      "Total errors are: 6\n",
      "\n",
      "Accuracy for dropped sepallength and petallength is: 92.66666666666666 %\n",
      "Errors for Iris-versicolor is: 1\n",
      "Errors for Iris-virginica is: 4\n",
      "Errors for Iris-setosa is: 6\n",
      "Total errors are: 11\n",
      "\n",
      "Accuracy for dropped sepallength and petalwidth is: 92.66666666666666 %\n",
      "Errors for Iris-virginica is: 0\n",
      "Errors for Iris-setosa is: 2\n",
      "Errors for Iris-versicolor is: 9\n",
      "Total errors are: 11\n",
      "\n",
      "Accuracy for dropped sepalwidth and petallength is: 81.33333333333333 %\n",
      "Errors for Iris-setosa is: 0\n",
      "Errors for Iris-versicolor is: 13\n",
      "Errors for Iris-virginica is: 15\n",
      "Total errors are: 28\n",
      "\n",
      "Accuracy for dropped sepalwidth and petalwidth is: 88.0 %\n",
      "Errors for Iris-versicolor is: 0\n",
      "Errors for Iris-setosa is: 5\n",
      "Errors for Iris-virginica is: 13\n",
      "Total errors are: 18\n",
      "\n",
      "Accuracy for dropped petallength and petalwidth is: 82.0 %\n",
      "Errors for Iris-setosa is: 0\n",
      "Errors for Iris-virginica is: 12\n",
      "Errors for Iris-versicolor is: 15\n",
      "Total errors are: 27\n",
      "\n",
      "[ 1.  1.  1.  1.  1.  1.  6. 78.] Flower number: 78 was wrongly clustered 6 times.\n",
      "[  1.   1.   1.   1.   1.   1.   6. 107.] Flower number: 107 was wrongly clustered 6 times.\n",
      "[  0.   0.   1.   1.   1.   1.   4. 114.] Flower number: 114 was wrongly clustered 4 times.\n",
      "[  1.   1.   1.   1.   1.   1.   6. 120.] Flower number: 120 was wrongly clustered 6 times.\n",
      "[  0.   0.   1.   1.   1.   1.   4. 122.] Flower number: 122 was wrongly clustered 4 times.\n",
      "[  0.   0.   1.   1.   1.   1.   4. 124.] Flower number: 124 was wrongly clustered 4 times.\n",
      "[  1.   0.   1.   1.   1.   1.   5. 127.] Flower number: 127 was wrongly clustered 5 times.\n",
      "[  0.   0.   1.   1.   1.   1.   4. 128.] Flower number: 128 was wrongly clustered 4 times.\n",
      "[  1.   0.   1.   1.   1.   1.   5. 139.] Flower number: 139 was wrongly clustered 5 times.\n"
     ]
    }
   ],
   "source": [
    "Gamma_data = data.copy()\n",
    "names = [\"sepallength\", \"sepalwidth\", \"petallength\",\"petalwidth\"]\n",
    "gamma_n_classes = 3\n",
    "errorIndex = 0\n",
    "errorMatrix = np.zeros((150, 8)) \n",
    "\n",
    "for i in range(150):\n",
    "    errorMatrix[i,7] = i + 1\n",
    "\n",
    "for i in range(data.shape[1] - 2):\n",
    "    for j in range(i+1,data.shape[1] - 1):\n",
    "        classes = Gamma_data['class'].copy()\n",
    "        temp_data = Gamma_data.copy()\n",
    "        Gamma_train = temp_data.drop([names[i],names[j],'class'], axis=1)\n",
    "\n",
    "        kmeans = KMeans(n_clusters=gamma_n_classes, n_init=10)\n",
    "        kmeans.fit(Gamma_train)\n",
    "\n",
    "        y_pred = kmeans.fit_predict(Gamma_train)\n",
    "        \n",
    "        label1 = most_frequent(y_pred[:50])\n",
    "        label2 = most_frequent(y_pred[50:100])\n",
    "        label3 = most_frequent(y_pred[100:150])\n",
    "        classesNames = ['', '', '']\n",
    "        \n",
    "        for t in range(classes.size):\n",
    "            if classes[t] == \"Iris-setosa\":\n",
    "                classes[t] = label1[0][0]\n",
    "                classesNames[label1[0][0]] = \"Iris-setosa\"\n",
    "            if classes[t] == \"Iris-versicolor\":\n",
    "                classes[t] = label2[0][0]\n",
    "                classesNames[label2[0][0]] = \"Iris-versicolor\"\n",
    "            if classes[t] == \"Iris-virginica\":\n",
    "                classes[t] = label3[0][0]\n",
    "                classesNames[label3[0][0]] = \"Iris-virginica\"\n",
    "        errors = [0, 0, 0, 0]\n",
    "        \n",
    "        \n",
    "        \n",
    "        for r in range(y_pred.size):\n",
    "            if(r < 50):\n",
    "                if y_pred[r] != classes[r]:\n",
    "                    errors[0] = errors[0] + 1\n",
    "                    errorMatrix[r, errorIndex] = errorMatrix[r, errorIndex] + 1\n",
    "            if(r >= 50 and r < 100):\n",
    "                if y_pred[r] != classes[r]:\n",
    "                    errors[1] = errors[1] + 1\n",
    "                    errorMatrix[r, errorIndex] = errorMatrix[r, errorIndex] + 1\n",
    "            if(r >= 100 and r < 150):\n",
    "                if y_pred[r] != classes[r]:\n",
    "                    errors[2] = errors[2] + 1\n",
    "                    errorMatrix[r, errorIndex] = errorMatrix[r, errorIndex] + 1\n",
    "                    \n",
    "        errors[3] = errors[0] + errors[1] + errors[2]\n",
    "        errorIndex = errorIndex + 1\n",
    "\n",
    "        print('Accuracy for dropped', names[i], 'and', names[j], 'is:', ((y_pred.size - errors[3])/y_pred.size)*100, '%')\n",
    "        print('Errors for', classesNames[0], 'is:', errors[0])\n",
    "        print('Errors for', classesNames[1], 'is:', errors[1])\n",
    "        print('Errors for', classesNames[2], 'is:', errors[2])\n",
    "        print('Total errors are:', errors[3]) \n",
    "        print('')\n",
    "\n",
    "for i in range(150):\n",
    "    errorMatrix[i, 6] =  errorMatrix[i, 0] + errorMatrix[i, 1] + errorMatrix[i, 2] + errorMatrix[i, 3] + errorMatrix[i, 4] + errorMatrix[i, 5]\n",
    "\n",
    "for i in errorMatrix:\n",
    "    if i[6] >= 4:\n",
    "        print(i, 'Flower number:', int(i[7]), 'was wrongly clustered', int(i[6]), 'times.')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get the best case, subspace, when we drop sepallength and sepalwidth, accuracy is: 96.0%. We can see that the most troublesome iris is the Iris-versicolor, it is always most often wrongly classified, in every subspace. We can also see that flowers number 78, 107 and 120 are wrongly classified in every subspace. Above one can also see other flowers that is pretty hard to classify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
