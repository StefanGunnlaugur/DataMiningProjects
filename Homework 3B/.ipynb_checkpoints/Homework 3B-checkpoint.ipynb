{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group 4\n",
    "\n",
    "#### Team members:\n",
    "\n",
    "Arngrímur Einarsson  \n",
    "Guðmundur Orri Pálsson  \n",
    "Nick Geerjens  \n",
    "Stefán Gunnlaugur Jónsson  \n",
    "Troy The Legend  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import time\n",
    "import math\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = pd.read_excel(\"HW3Avalidate.xlsx\") \n",
    "data_train = pd.read_excel(\"HW3Atrain.xlsx\")\n",
    "\n",
    "X_train = data_train.copy()\n",
    "X_train = X_train.drop('y', axis=1)\n",
    "Y_train = data_train.copy()\n",
    "Y_train = Y_train.drop('X_0', axis=1)\n",
    "Y_train = Y_train.drop('X_1', axis=1)\n",
    "\n",
    "X_test = data_test.copy()\n",
    "X_test = X_test.drop('y', axis=1)\n",
    "Y_test = data_test.copy()\n",
    "Y_test = Y_test.drop('X_0', axis=1)\n",
    "Y_test = Y_test.drop('X_1', axis=1)\n",
    "\n",
    "\n",
    "x_0_min = min(X_train['X_0'])\n",
    "x_0_max = max(X_train['X_0'])\n",
    "\n",
    "x_1_min = min(X_train['X_1'])\n",
    "x_1_max = max(X_train['X_1'])\n",
    "\n",
    "for i in range(len(X_train['X_0'])):\n",
    "    X_train['X_0'][i] = (X_train['X_0'][i]-x_0_min)/(x_0_max-x_0_min) #formula used for Min-Max\n",
    "    X_train['X_1'][i] = (X_train['X_1'][i]-x_1_min)/(x_1_max-x_1_min) #formula used for Min-Max\n",
    "    \n",
    "for i in range(len(X_test['X_0'])):\n",
    "    X_test['X_0'][i] = (X_test['X_0'][i]-x_0_min)/(x_0_max-x_0_min) #formula used for Min-Max\n",
    "    X_test['X_1'][i] = (X_test['X_1'][i]-x_1_min)/(x_1_max-x_1_min) #formula used for Min-Max\n",
    "'''\n",
    "training_data = []\n",
    "for i in range(len(X_train['X_0'])):\n",
    "    training_data.append([round(X_train['X_0'][i], 2),round(X_train['X_1'][i],2)])\n",
    "    \n",
    "testing_data = []\n",
    "for i in range(len(X_test['X_0'])):\n",
    "    testing_data.append([round(X_test['X_0'][i],2),round(X_test['X_1'][i],2)])\n",
    "\n",
    "y_train = []\n",
    "\n",
    "for i in range(len(Y_train['y'])):\n",
    "    y_train.append([Y_train['y'][i]])\n",
    "    \n",
    "y_test = []\n",
    "\n",
    "for i in range(len(Y_test['y'])):\n",
    "    y_test.append([Y_test['y'][i]])\n",
    "'''\n",
    "\n",
    "training_data = np.empty((0,2), int)\n",
    "for i in range(len(X_train['X_0'])):\n",
    "    training_data = np.append(training_data, np.array([[X_train['X_0'][i],X_train['X_1'][i]]]), axis=0)\n",
    "    \n",
    "testing_data = np.empty((0,2), int)\n",
    "for i in range(len(X_test['X_0'])):\n",
    "    testing_data = np.append(testing_data, np.array([[X_test['X_0'][i],X_test['X_1'][i]]]), axis=0)\n",
    "\n",
    "\n",
    "\n",
    "y_train = np.empty((0,1), int)\n",
    "for i in range(len(Y_train['y'])):\n",
    "    y_train = np.append(y_train, np.array([[Y_train['y'][i]]]), axis=0)\n",
    "\n",
    "y_test = np.empty((0,1), int)\n",
    "for i in range(len(Y_test['y'])):\n",
    "    y_test = np.append(y_test, np.array([[Y_test['y'][i]]]), axis=0)\n",
    "\n",
    "np_testing_y = np.array(y_test)\n",
    "np_testing_x = np.array(testing_data)\n",
    "np_training_x = np.array(training_data)\n",
    "np_training_y = np.array(y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "data type not understood",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-1f96deacd5d2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 304\u001b[0;31m \u001b[0mNN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minit_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    305\u001b[0m \u001b[0mrun_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-1f96deacd5d2>\u001b[0m in \u001b[0;36minit_net\u001b[0;34m(hidden_layers)\u001b[0m\n\u001b[1;32m    295\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0minit_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m     \u001b[0mNN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNeaural_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 297\u001b[0;31m     \u001b[0mNN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNetwork_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    298\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_layers\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m         \u001b[0mNN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNetwork_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-1f96deacd5d2>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, number_input, number_neurons, use_standard_activation)\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;31m#self.weights = (np.random.rand(number_input,number_neurons)*2) - 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;31m#self.weights = np.random.rand(number_input,number_neurons)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumber_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnumber_neurons\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumber_neurons\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_activation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: data type not understood"
     ]
    }
   ],
   "source": [
    "class Network_layer:\n",
    "    \n",
    "    def __init__(self, number_input, number_neurons, use_standard_activation):\n",
    "        #self.size = size\n",
    "        #self.weights = (np.random.rand(number_input,number_neurons)*2) - 1\n",
    "        #self.weights = np.random.rand(number_input,number_neurons)\n",
    "        self.weights = (np.zeros((number_input,number_neurons))\n",
    "        self.bias = np.zeros(number_neurons)\n",
    "        self.last_activation = None\n",
    "        self.use_standard_activation = use_standard_activation\n",
    "        self.error = None\n",
    "        self.delta = None\n",
    "        \n",
    "    def activate(self, x):\n",
    "        r = np.dot(x, self.weights) + self.bias\n",
    "        self.last_activation = self.activation(r)\n",
    "            \n",
    "        return self.last_activation\n",
    "\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    def tanh(self, x):\n",
    "        return np.tanh(x)\n",
    "    \n",
    "    def activation(self, x):\n",
    "        #relu\n",
    "        if self.use_standard_activation:\n",
    "            x[x<0] = 0\n",
    "            return x\n",
    "        #tanh\n",
    "        return self.tanh(x)\n",
    "    \n",
    "    def activation_derivative(self, x):\n",
    "        #relu derivative\n",
    "        if self.use_standard_activation:\n",
    "            x[x>0] = 1\n",
    "            x[x<0] = 0\n",
    "            return x\n",
    "        #tanh derivative\n",
    "        return 1.0 - np.tanh(x)**2\n",
    "    \n",
    "\n",
    "class Neaural_net:\n",
    "    layers = []\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "    \n",
    "    def add_layer(self, layer):\n",
    "        self.layers.append(layer)\n",
    "    \n",
    "    def feed_forward(self, X):\n",
    "        for layer in self.layers:\n",
    "            X = layer.activate(X)\n",
    "        return X\n",
    "\n",
    "    def softmax(self,X):\n",
    "        exps = np.exp(X)\n",
    "        return exps / np.sum(exps)\n",
    "    \n",
    "    def cross_entropy(self, X,y):\n",
    "        m = y.shape[0]\n",
    "        p = self.softmax(X)\n",
    "        \n",
    "        likelihood = -np.log(p[range(m), y.argmax(axis=1)])\n",
    "        loss = np.sum(likelihood) / m\n",
    "        return loss\n",
    "        \n",
    "    def logloss(self, true_label, predicted, eps=1e-15):\n",
    "        p = np.clip(predicted, eps, 1 - eps)\n",
    "        if true_label == 1:\n",
    "            return -math.log(p)\n",
    "        else:\n",
    "            return -math.log(1 - p) \n",
    "        \n",
    "    def mae(self, targets, predictions):\n",
    "        differences = predictions - targets\n",
    "        absolute_differences = np.absolute(differences)\n",
    "        mean_absolute_differences = absolute_differences.mean()\n",
    "        return mean_absolute_differences\n",
    "        \n",
    "    def mean_squared_error(self, actual, predicted):\n",
    "        sum_square_error = 0.0\n",
    "        for i in range(len(actual)):\n",
    "            sum_square_error += (actual[i] - predicted[i])**2.0\n",
    "        mean_square_error = sum_square_error / len(actual)\n",
    "\n",
    "        return mean_square_error\n",
    "        \n",
    "    def checkPrediction(self, y, pred):\n",
    "        pred = np.around(pred)\n",
    "        if pred == y:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "               \n",
    "        \n",
    "    def backpropagation(self, X, y, learning_rate):\n",
    "        output = self.feed_forward(X)\n",
    "        average_activation = None\n",
    "        correct_predictions = 0\n",
    "        for i in reversed(range(len(self.layers))):\n",
    "            layer = self.layers[i]\n",
    "            all_deltas = []\n",
    "            last_activations = []\n",
    "            for j in range(len(output)):\n",
    "                if layer == self.layers[-1]:\n",
    "                    layer.error =  y[j] - output[j]\n",
    "                    all_deltas.append(layer.error * layer.activation_derivative(output[j]))\n",
    "                    last_activations.append(layer.activation_derivative(output[j]))\n",
    "                    correct_predictions = correct_predictions + self.checkPrediction(y[j], output[j])\n",
    "                else:\n",
    "                    next_layer = self.layers[i + 1]\n",
    "                    layer.error = np.dot(next_layer.weights, next_layer.delta)\n",
    "                    all_deltas.append(layer.error * layer.activation_derivative(layer.last_activation[j]))\n",
    "                    last_activations.append(layer.activation_derivative(layer.last_activation[j]))\n",
    "                    \n",
    "            average_delta = sum(all_deltas)/float(len(output))\n",
    "            average_activation = sum(last_activations)/float(len(output))\n",
    "            layer.delta = average_delta\n",
    "    \n",
    "        # Update the weights\n",
    "        for j in range(len(output)):\n",
    "            for i in range(len(self.layers)):\n",
    "                layer = self.layers[i]\n",
    "                input_to_use = np.atleast_2d(X[j] if i == 0 else self.layers[i - 1].last_activation[j])\n",
    "                layer.weights += layer.delta * input_to_use.T * learning_rate\n",
    "        return correct_predictions\n",
    "            \n",
    "    \n",
    "    \n",
    "    def get_batch(self, inputs, targets, batchsize, shuffle=False):\n",
    "        assert len(inputs) == len(targets)\n",
    "        if shuffle:\n",
    "            indices = np.random.permutation(len(inputs))\n",
    "        for start in range(0, len(inputs) - batchsize + 1, batchsize):\n",
    "            if shuffle:\n",
    "                excerpt = indices[start:start + batchsize]\n",
    "            else:\n",
    "                excerpt = slice(start, start + batchsize)\n",
    "            yield inputs[excerpt], targets[excerpt] \n",
    "\n",
    "            \n",
    "    def train(self, X, y, learning_rate, max_epochs, batchsize):\n",
    "        mses = []\n",
    "        training_accuracy = []\n",
    "        for i in range(max_epochs):\n",
    "            correct_per_epoch = 0\n",
    "            for x_batch,y_batch in self.get_batch(X,y,batchsize=batchsize,shuffle=False):\n",
    "                correct_predictions_batch = self.backpropagation(x_batch,y_batch, learning_rate)\n",
    "                correct_per_epoch = correct_per_epoch + correct_predictions_batch\n",
    "\n",
    "            if i % 100 == 0:\n",
    "                mse = np.mean(np.square(y - NN.feed_forward(X)))\n",
    "                mses.append(mse)\n",
    "                training_accuracy.append((correct_per_epoch/len(X))*100)\n",
    "        return mses, training_accuracy \n",
    "    \n",
    "    \n",
    "    \n",
    "    def test(self, X, y_true):\n",
    "        correct = 0\n",
    "        TT = 0 \n",
    "        TF = 0\n",
    "        FT = 0\n",
    "        FF = 0\n",
    "        mses = []\n",
    "        forward_passes = []\n",
    "        y_truth_values = []\n",
    "        for i in range(len(X)):\n",
    "            forward_pass = self.feed_forward(X[i])\n",
    "            forward_passes.append(forward_pass)\n",
    "            pred = np.around(forward_pass)\n",
    "            y = y_true[i]\n",
    "            y_truth_values.append(y)\n",
    "            if pred == y:\n",
    "                correct = correct + 1\n",
    "            if pred == 1 and y == 1:\n",
    "                TT = TT + 1\n",
    "            if pred == 1 and y == 0:\n",
    "                FT = FT + 1\n",
    "            if pred == 0 and y == 1:\n",
    "                FF = FF + 1\n",
    "            if pred == 0 and y == 0:\n",
    "                TF = TF + 1\n",
    "            confusion = [TT, TF, FT, FF]\n",
    "        mse = np.mean(np.square(np.array(y_truth_values) - np.array(forward_passes)))\n",
    "        mses.append(mse)\n",
    "        return correct/len(X), confusion, mses\n",
    "\n",
    "def should_break(last_mse, mses, lowest_mse):\n",
    "    stop = False\n",
    "    mse_same = True\n",
    "    for i in range(len(last_mse)):\n",
    "        if round(last_mse[i],4) != round(mses[0],4):\n",
    "            mse_same = False\n",
    "    if mse_same:\n",
    "        print(\"MSE has converged!\")\n",
    "        stop = True\n",
    "    if round(mses[0], 3) < 0.07:\n",
    "        print(\"Error is sufficiently low!\")\n",
    "        stop = True\n",
    "    if round(mses[0],4) - lowest_mse > 0.05:\n",
    "        print(\"MSE increased!\")\n",
    "        stop = True\n",
    "    return stop\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def run_training(NN, learning_rate = 0.009, batchsize = 5, number_test = 25, number_epoc = 50):\n",
    "    # Train the neural networks\n",
    "    accuracy = []\n",
    "    iteration = []\n",
    "    total_error = []\n",
    "    total_mses = []\n",
    "    total_training_accuracy = []\n",
    "    last_mse = [0,0,0,0,0]\n",
    "    a = 0\n",
    "    total_time = 0\n",
    "    max_accuracy = 0\n",
    "    best_NN = copy.copy(NN)\n",
    "    best_confusion = [0,0,0,0]\n",
    "    best_learning_rate = learning_rate\n",
    "    recall = []\n",
    "    precision = []\n",
    "    lowest_mse = 100\n",
    "    \n",
    "    for i in range(1,number_test+1):\n",
    "        start = time.time()\n",
    "        errors, training_accuracy = NN.train(np_training_x, np_training_y, learning_rate, number_epoc, batchsize)\n",
    "        end = time.time()\n",
    "        total_time = total_time + (end-start)\n",
    "        total_error.append(errors)\n",
    "        total_training_accuracy.append(training_accuracy)\n",
    "        iteration.append(i*number_epoc)\n",
    "        accuracy_one, confusion, mses = NN.test(np_testing_x, np_testing_y)\n",
    "        total_mses.append(mses)\n",
    "        last_mse[a] = round(mses[0],4)\n",
    "        a = a + 1\n",
    "        if a > 4:\n",
    "            a = 0\n",
    "\n",
    "        if(accuracy_one > max_accuracy):\n",
    "            max_accuracy = copy.copy(accuracy_one)\n",
    "            best_NN = copy.deepcopy(NN)\n",
    "            best_confusion = copy.copy(confusion)\n",
    "            best_learning_rate = copy.copy(learning_rate)\n",
    "        if mses[0] < lowest_mse:\n",
    "            lowest_mse = mses[0]\n",
    "        print('Training accuracy-iteration: %.2f%%' % (training_accuracy[-1]))\n",
    "        print('Testing accuracy-iteration: %.2f%%' % (accuracy_one*100), \"Learning rate --> \"+ str(learning_rate))\n",
    "        print('Confusion matrix ---> TrueOne:',confusion[0] ,'TrueZero:',confusion[1], 'FalseOne:',confusion[2] , 'FalseZero',confusion[3] )\n",
    "        print(\"MES: \", mses[0])\n",
    "        print()\n",
    "        #recall_tmp = round(confusion[0]/(confusion[0]+confusion[3]), 2)\n",
    "        #precision_tmp = round(confusion[0]/(confusion[0]+confusion[2]), 2)\n",
    "        #recall.append(recall_tmp)\n",
    "        #print('Recall=', recall_tmp)\n",
    "        #precision.append(precision_tmp)\n",
    "        #print('Precision=', precision_tmp)\n",
    "        accuracy.append(accuracy_one*100)\n",
    "        if should_break(last_mse, mses, lowest_mse):\n",
    "            break;\n",
    "        learning_rate = learning_rate*0.97\n",
    "        \n",
    "    print()\n",
    "    print(\"Average training time for network running \" + str(number_epoc) + \" iterations with batch size \" + str(batchsize) + \" --->\" + str(total_time/number_test)+ \"s\")\n",
    "    plt.plot(iteration,accuracy, label='Validation accuracy')\n",
    "    #plt.plot(iteration, total_error, label='Mean squared error for training set')\n",
    "    #plt.plot(iteration, total_mses, label='Mean squared error for validation set')\n",
    "    plt.plot(iteration, total_training_accuracy, label='Training accuracy')\n",
    "    #plt.plot(iteration, recall, label='Recall')\n",
    "    #plt.plot(iteration, precision, label='Precision')\n",
    "    plt.xlabel('iterations')\n",
    "    plt.ylabel('Accuracy & MSE(*100)')\n",
    "    plt.axis((number_epoc,number_epoc*number_test,0,100))\n",
    "    plt.title('')\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    accuracy_one, confusion, mses = best_NN.test(np_testing_x, np_testing_y)\n",
    "    print(\"------------------------\")\n",
    "    print('Accuracy-best: %.2f%%' % (accuracy_one*100), \"Learning rate --> \"+ str(best_learning_rate))\n",
    "    print('TrueOne:',confusion[0] ,'TrueZero:',confusion[1], 'FalseOne:',confusion[2] , 'FalseZero',confusion[3] )\n",
    "    print()\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "def init_net(hidden_layers):\n",
    "    NN = Neaural_net()\n",
    "    NN.add_layer(Network_layer(2, 10, False))\n",
    "    for i in range(hidden_layers - 1):\n",
    "        NN.add_layer(Network_layer(10, 10, False))\n",
    "    NN.add_layer(Network_layer(10,1, False))\n",
    "    return NN\n",
    "\n",
    "\n",
    "NN = init_net(2)\n",
    "run_training(NN)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. (10 points) Activation and Loss functions. \n",
    "Please choose suitable activation functions φ(0), φ(1), φ(2) and a suitable Loss function to perform the task. Report and justify your choices in the report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh(self, x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def tanh_derivative(self, x)\n",
    "    return 1.0 - np.tanh(x)**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We decided to use the tanh activation function in all the layers, we tried to use sigmoid and ReLu but we got the best results using ReLu. \n",
    "\n",
    "To calculate the error we subtracted the output from our net from the true value from the set. Then we used Mean Squared error to determine the net's accuracy. $$MSE = \\frac{1}{n}\\sum_{i=1}^{n} (y_i-\\bar{y}_i)^2$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. (10 points) Learning rate, batch size, initialization. \n",
    "Please choose a suitable learning rate, batch size, initialization of the parameter values, and any other setting you may need. Discuss and justify your choices in the report."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We started with the learning rate as 0.009 and after each test the learning rate was reduced by 1%. We tried various batch sizes, what worked best was relatively small batch sizes so we ended using \n",
    "$$batchsize=5$$\n",
    "We initialized our weights as a random number from 0 to 1 and our biases as 0. We read that it is best to initialize biases as zero, it also didn´t give good results when we tried random to initilaze the biases. We knew that using random initialization for the weights is better than using zeroes. We tried some other initilizations such as from 0 to 0.5 or -0.5 to 0.5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. (10 points) Training. \n",
    "Make plots with the loss function computed over the training set and over the validation set. Stop the training when the error is small enough. Justify your stopping criterium. Report the final accuracy obtained and the confusion matrix on the validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(iteration, total_error, label='Mean squared error for training set')\n",
    "plt.plot(iteration, total_mses, label='Mean squared error for validation set')\n",
    "plt.xlabel('Training Iterations')\n",
    "plt.ylabel('MSE')\n",
    "plt.axis((number_epoc,number_epoc*number_test,0,1))\n",
    "plt.title('')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training stops if any of the following three events occure.\n",
    "    i) The mean square error drops below 0.07.\n",
    "    ii) The mean square error drastically increases that is MSE_new - MSE_last > 0.05.\n",
    "    iii) The mean square error converges and doesn´t change for 5 rounds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get the final confusion matrix: $$\\text{Confusion} = \\begin{bmatrix} \\text{True One} & \\text{True Zero} \\\\ \\text{False One} & \\text{False Zero} \\end{bmatrix} = \\begin{bmatrix} 39&36\\\\5&2\\end{bmatrix}$$\n",
    "\n",
    "The final accuracy is: $$\\text{Accuracy} = 91.46\\%$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. (20 points) Implementation. \n",
    "We will run and check the uploaded Python file. To obtain the points for this subproblem, the Python file has to run (no errors) and the MLP model and the Backpropagation algorithm have to be implemented completely from scratch by you. You are not allowed to use any library which implements MLP models, but you are allowed to use auxiliary libraries, e.g. Numpy, Matplotlib, Pandas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code is above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Peer Review paragraph (0 points)\n",
    "Finally, each group member must write a single paragraph outlining their opinion on the work distri- bution within the group. Did every group member contribute equally? Did you split up tasks in a fair manner, or jointly worked through the exercises. Do you think that some members of your group deserve a different grade from others?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
