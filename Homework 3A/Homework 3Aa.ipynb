{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from tqdm import trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = pd.read_excel(\"HW3Avalidate.xlsx\") \n",
    "data_train = pd.read_excel(\"HW3Atrain.xlsx\")\n",
    "\n",
    "X_train = data_train.copy()\n",
    "X_train = X_train.drop('y', axis=1)\n",
    "Y_train = data_train.copy()\n",
    "Y_train = Y_train.drop('X_0', axis=1)\n",
    "Y_train = Y_train.drop('X_1', axis=1)\n",
    "\n",
    "X_test = data_test.copy()\n",
    "X_test = X_test.drop('y', axis=1)\n",
    "Y_test = data_test.copy()\n",
    "Y_test = Y_test.drop('X_0', axis=1)\n",
    "Y_test = Y_test.drop('X_1', axis=1)\n",
    "\n",
    "\n",
    "x_0_min = min(X_train['X_0'])\n",
    "x_0_max = max(X_train['X_0'])\n",
    "\n",
    "x_1_min = min(X_train['X_1'])\n",
    "x_1_max = max(X_train['X_1'])\n",
    "\n",
    "test_x_0_min = min(X_test['X_0'])\n",
    "test_x_0_max = max(X_test['X_0'])\n",
    "\n",
    "test_x_1_min = min(X_test['X_1'])\n",
    "test_x_1_max = max(X_test['X_1'])\n",
    "\n",
    "for i in range(len(X_train['X_0'])):\n",
    "    X_train['X_0'][i] = (X_train['X_0'][i]-x_0_min)/(x_0_max-x_0_min) #formula used for Min-Max\n",
    "    X_train['X_1'][i] = (X_train['X_1'][i]-x_1_min)/(x_1_max-x_1_min) #formula used for Min-Max\n",
    "    \n",
    "for i in range(len(X_test['X_0'])):\n",
    "    X_test['X_0'][i] = (X_test['X_0'][i]-x_0_min)/(x_0_max-x_0_min) #formula used for Min-Max\n",
    "    X_test['X_1'][i] = (X_test['X_1'][i]-x_1_min)/(x_1_max-x_1_min) #formula used for Min-Max\n",
    "'''\n",
    "training_data = []\n",
    "for i in range(len(X_train['X_0'])):\n",
    "    training_data.append([round(X_train['X_0'][i], 2),round(X_train['X_1'][i],2)])\n",
    "    \n",
    "testing_data = []\n",
    "for i in range(len(X_test['X_0'])):\n",
    "    testing_data.append([round(X_test['X_0'][i],2),round(X_test['X_1'][i],2)])\n",
    "\n",
    "y_train = []\n",
    "\n",
    "for i in range(len(Y_train['y'])):\n",
    "    y_train.append([Y_train['y'][i]])\n",
    "    \n",
    "y_test = []\n",
    "\n",
    "for i in range(len(Y_test['y'])):\n",
    "    y_test.append([Y_test['y'][i]])\n",
    "'''\n",
    "\n",
    "training_data = np.empty((0,2), int)\n",
    "for i in range(len(X_train['X_0'])):\n",
    "    training_data = np.append(training_data, np.array([[X_train['X_0'][i],X_train['X_1'][i]]]), axis=0)\n",
    "    \n",
    "testing_data = np.empty((0,2), int)\n",
    "for i in range(len(X_test['X_0'])):\n",
    "    testing_data = np.append(testing_data, np.array([[X_test['X_0'][i],X_test['X_1'][i]]]), axis=0)\n",
    "\n",
    "\n",
    "\n",
    "y_train = np.empty((0,1), int)\n",
    "for i in range(len(Y_train['y'])):\n",
    "    y_train = np.append(y_train, np.array([[Y_train['y'][i]]]), axis=0)\n",
    "\n",
    "y_test = np.empty((0,1), int)\n",
    "for i in range(len(Y_test['y'])):\n",
    "    y_test = np.append(y_test, np.array([[Y_test['y'][i]]]), axis=0)\n",
    "\n",
    "np_testing_y = np.array(y_test)\n",
    "np_testing_x = np.array(testing_data)\n",
    "np_training_x = np.array(training_data)\n",
    "np_training_y = np.array(y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24.38\n",
      "Accuracy-iteration: 84.15% Learning rate --> 0.0099\n",
      "9.25\n",
      "Accuracy-iteration: 84.15% Learning rate --> 0.009801\n",
      "7.88\n",
      "Accuracy-iteration: 89.02% Learning rate --> 0.00970299\n",
      "7.03\n",
      "Accuracy-iteration: 91.46% Learning rate --> 0.0096059601\n",
      "6.15\n",
      "Accuracy-iteration: 91.46% Learning rate --> 0.009509900499\n",
      "5.68\n",
      "Accuracy-iteration: 91.46% Learning rate --> 0.00941480149401\n",
      "5.42\n",
      "Accuracy-iteration: 92.68% Learning rate --> 0.0093206534790699\n",
      "5.15\n",
      "Accuracy-iteration: 92.68% Learning rate --> 0.0092274469442792\n",
      "4.92\n",
      "Accuracy-iteration: 92.68% Learning rate --> 0.009135172474836408\n",
      "4.75\n",
      "Accuracy-iteration: 93.90% Learning rate --> 0.009043820750088045\n",
      "4.57\n",
      "Accuracy-iteration: 93.90% Learning rate --> 0.008953382542587164\n",
      "4.31\n",
      "Accuracy-iteration: 95.12% Learning rate --> 0.008863848717161293\n"
     ]
    }
   ],
   "source": [
    "class Network_layer:\n",
    "    \n",
    "    def __init__(self, number_input, number_neurons, use_standard_activation):\n",
    "        #self.size = size\n",
    "        self.weights = np.random.rand(number_input,number_neurons)\n",
    "        self.bias = np.zeros(number_neurons)\n",
    "        self.last_activation = None\n",
    "        self.use_standard_activation = use_standard_activation\n",
    "        self.error = None\n",
    "        self.delta = None\n",
    "        \n",
    "    def activate(self, x):\n",
    "        r = np.dot(x, self.weights) + self.bias\n",
    "        if self.use_standard_activation:\n",
    "            self.last_activation = self.activation(r)\n",
    "        else: \n",
    "            self.last_activation = self.tanh(r)\n",
    "            \n",
    "        return self.last_activation\n",
    "    \n",
    "    def activation(self, x):\n",
    "        #relu\n",
    "        x[x<0] = 0\n",
    "        return x\n",
    "                \n",
    "    def tanh(self, x):\n",
    "        #tanh\n",
    "        return np.tanh(x)\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    def activation_derivative(self, x):\n",
    "        if self.use_standard_activation:\n",
    "            x[x>0] = 1\n",
    "            x[x<0] = 0\n",
    "            return x\n",
    "        return 1.0 - np.tanh(x)**2\n",
    "        '''\n",
    "        if self.use_standard_activation:\n",
    "            return 1.0 - np.tanh(x)**2\n",
    "        return x * (1 - x)\n",
    "        '''\n",
    "    \n",
    "\n",
    "class Neaural_net:\n",
    "    layers = []\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "    \n",
    "    def add_layer(self, layer):\n",
    "        self.layers.append(layer)\n",
    "    \n",
    "    def feed_forward(self, X):\n",
    "        for layer in self.layers:\n",
    "            X = layer.activate(X)\n",
    "        return X\n",
    "\n",
    "        \n",
    "    def backpropagation(self, X, y, learning_rate):\n",
    "        #print(y)\n",
    "        #print(X)\n",
    "        output = self.feed_forward(X)\n",
    "        #for i in reversed(range(len(self.layers))):\n",
    "        #    print(self.layers[i].weights.shape)\n",
    "\n",
    "        #for j in range(len(output)):\n",
    "        for i in reversed(range(len(self.layers))):\n",
    "            #print(y[0])\n",
    "            layer = self.layers[i]\n",
    "            if layer == self.layers[-1]:\n",
    "                layer.error = y - output\n",
    "                #print(layer.error)\n",
    "                #layer.error = np.square(y - NN.feed_forward(X))\n",
    "                layer.delta = layer.error * layer.activation_derivative(output)\n",
    "                #print(layer.delta)\n",
    "            else:\n",
    "                next_layer = self.layers[i + 1]\n",
    "                layer.error = np.dot(next_layer.weights, next_layer.delta)\n",
    "                layer.delta = layer.error * layer.activation_derivative(layer.last_activation)\n",
    "                #print('sdf', layer.delta)\n",
    "\n",
    "\n",
    "        # Update the weights\n",
    "        for i in range(len(self.layers)):\n",
    "            layer = self.layers[i]\n",
    "            input_to_use = np.atleast_2d(X if i == 0 else self.layers[i - 1].last_activation)\n",
    "            layer.weights += layer.delta * input_to_use.T * learning_rate\n",
    "            \n",
    "    \n",
    "    \n",
    "    def iterate_minibatches(self, inputs, targets, batchsize, shuffle=False):\n",
    "        assert len(inputs) == len(targets)\n",
    "        if shuffle:\n",
    "            indices = np.random.permutation(len(inputs))\n",
    "        for start_idx in trange(0, len(inputs) - batchsize + 1, batchsize):\n",
    "            if shuffle:\n",
    "                excerpt = indices[start_idx:start_idx + batchsize]\n",
    "            else:\n",
    "                excerpt = slice(start_idx, start_idx + batchsize)\n",
    "            yield inputs[excerpt], targets[excerpt]\n",
    "\n",
    "    '''\n",
    "    from IPython.display import clear_output\n",
    "    train_log = []\n",
    "    val_log = []\n",
    "    for epoch in range(25):\n",
    "        for x_batch,y_batch in iterate_minibatches(X_train,y_train,batchsize=5,shuffle=True):\n",
    "            train(network,x_batch,y_batch)\n",
    "        train_log.append(np.mean(predict(network,X_train)==y_train))\n",
    "        val_log.append(np.mean(predict(network,X_test)==y_test))\n",
    "    '''    \n",
    "            \n",
    "    def train(self, X, y, learning_rate, max_epochs):\n",
    "        mses = []\n",
    "\n",
    "        for i in range(max_epochs):\n",
    "            #for x_batch,y_batch in self.iterate_minibatches(X,y,5,shuffle=True):\n",
    "\n",
    "             #   self.backpropagation(x_batch,y_batch, learning_rate)\n",
    "            for j in range(len(X)):\n",
    "                self.backpropagation(X[j], y[j], learning_rate)\n",
    "            if i % 100 == 0:\n",
    "                mse = np.mean(np.square(y - NN.feed_forward(X)))\n",
    "                mses.append(mse*100)\n",
    "        return mses\n",
    "    \n",
    "    \n",
    "    def test(self, X, y_true):\n",
    "        correct = 0\n",
    "        \n",
    "        for i in range(len(X)):\n",
    "            forward_pass = self.feed_forward(X[i])\n",
    "            if np.around(forward_pass) == y_true[i]:\n",
    "                correct = correct + 1\n",
    "        return correct/len(X)\n",
    "\n",
    "\n",
    "\n",
    "NN = Neaural_net()\n",
    "NN.add_layer(Network_layer(2, 10, False))\n",
    "NN.add_layer(Network_layer(10, 10, False))\n",
    "NN.add_layer(Network_layer(10, 10, False))\n",
    "NN.add_layer(Network_layer(10,1, False))\n",
    "\n",
    "\n",
    "# Train the neural networks\n",
    "accuracy = []\n",
    "iteration = []\n",
    "learning_rate = 0.01\n",
    "total_error = []\n",
    "number_test = 50\n",
    "number_epoc = 80\n",
    "last_mse = [0,0,0,0,0]\n",
    "a = 0\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "for i in range(1,number_test+1):\n",
    "    errors = NN.train(np_training_x, np_training_y, learning_rate, number_epoc)\n",
    "    total_error.append(errors)\n",
    "    learning_rate = learning_rate*0.99\n",
    "    last_mse[a] = round(errors[0],2)\n",
    "    print(last_mse[a])\n",
    "    a = a + 1\n",
    "    if a > 4:\n",
    "        a = 0\n",
    "    iteration.append(i*number_epoc)\n",
    "    accuracy_one = NN.test(np_testing_x, np_testing_y.flatten())\n",
    "    print('Accuracy-iteration: %.2f%%' % (accuracy_one*100), \"Learning rate --> \"+ str(learning_rate))\n",
    "    accuracy.append(accuracy_one*100)\n",
    "    mse_same = True\n",
    "    for i in range(len(last_mse)):\n",
    "        if round(last_mse[i],2) != round(errors[0],2):\n",
    "            mse_same = False\n",
    "    if mse_same:\n",
    "        print(\"MSE has converged!\")\n",
    "        break;\n",
    "\n",
    "\n",
    "plt.plot(iteration,accuracy)\n",
    "plt.plot(iteration, total_error)\n",
    "plt.xlabel('iterations')\n",
    "plt.ylabel('Accuracy & MSE(*100)')\n",
    "plt.axis((number_epoc,number_epoc*number_test,0,100))\n",
    "plt.title('Changes in Accuracy')\n",
    "plt.show()\n",
    "\n",
    "data = {'weights': []}\n",
    "data['weights'].append(NN.layers[0].weights.tolist())\n",
    "data['weights'].append(NN.layers[1].weights.tolist())\n",
    "data['weights'].append(NN.layers[2].weights.tolist())\n",
    "with open('data.txt', 'w') as outfile:\n",
    "    json.dump(data, outfile)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. (10 points) Activation and Loss functions. \n",
    "Please choose suitable activation functions φ(0), φ(1), φ(2) and a suitable Loss function to perform the task. Report and justify your choices in the report."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. (10 points) Learning rate, batch size, initialization. \n",
    "Please choose a suitable learning rate, batch size, initialization of the parameter values, and any other setting you may need. Discuss and justify your choices in the report."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. (10 points) Training. \n",
    "Make plots with the loss function computed over the training set and over the validation set. Stop the training when the error is small enough. Justify your stopping criterium. Report the final accuracy obtained and the confusion matrix on the validation dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. (20 points) Implementation. \n",
    "We will run and check the uploaded Python file. To obtain the points for this subproblem, the Python file has to run (no errors) and the MLP model and the Backpropagation algorithm have to be implemented completely from scratch by you. You are not allowed to use any library which implements MLP models, but you are allowed to use auxiliary libraries, e.g. Numpy, Matplotlib, Pandas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Peer Review paragraph (0 points)\n",
    "Finally, each group member must write a single paragraph outlining their opinion on the work distri- bution within the group. Did every group member contribute equally? Did you split up tasks in a fair manner, or jointly worked through the exercises. Do you think that some members of your group deserve a different grade from others?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
