{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import time\n",
    "import math\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = pd.read_excel(\"HW3Avalidate.xlsx\") \n",
    "data_train = pd.read_excel(\"HW3Atrain.xlsx\")\n",
    "\n",
    "X_train = data_train.copy()\n",
    "X_train = X_train.drop('y', axis=1)\n",
    "Y_train = data_train.copy()\n",
    "Y_train = Y_train.drop('X_0', axis=1)\n",
    "Y_train = Y_train.drop('X_1', axis=1)\n",
    "\n",
    "X_test = data_test.copy()\n",
    "X_test = X_test.drop('y', axis=1)\n",
    "Y_test = data_test.copy()\n",
    "Y_test = Y_test.drop('X_0', axis=1)\n",
    "Y_test = Y_test.drop('X_1', axis=1)\n",
    "\n",
    "\n",
    "x_0_min = min(X_train['X_0'])\n",
    "x_0_max = max(X_train['X_0'])\n",
    "\n",
    "x_1_min = min(X_train['X_1'])\n",
    "x_1_max = max(X_train['X_1'])\n",
    "\n",
    "test_x_0_min = min(X_test['X_0'])\n",
    "test_x_0_max = max(X_test['X_0'])\n",
    "\n",
    "test_x_1_min = min(X_test['X_1'])\n",
    "test_x_1_max = max(X_test['X_1'])\n",
    "\n",
    "for i in range(len(X_train['X_0'])):\n",
    "    X_train['X_0'][i] = (X_train['X_0'][i]-x_0_min)/(x_0_max-x_0_min) #formula used for Min-Max\n",
    "    X_train['X_1'][i] = (X_train['X_1'][i]-x_1_min)/(x_1_max-x_1_min) #formula used for Min-Max\n",
    "    \n",
    "for i in range(len(X_test['X_0'])):\n",
    "    X_test['X_0'][i] = (X_test['X_0'][i]-x_0_min)/(x_0_max-x_0_min) #formula used for Min-Max\n",
    "    X_test['X_1'][i] = (X_test['X_1'][i]-x_1_min)/(x_1_max-x_1_min) #formula used for Min-Max\n",
    "'''\n",
    "training_data = []\n",
    "for i in range(len(X_train['X_0'])):\n",
    "    training_data.append([round(X_train['X_0'][i], 2),round(X_train['X_1'][i],2)])\n",
    "    \n",
    "testing_data = []\n",
    "for i in range(len(X_test['X_0'])):\n",
    "    testing_data.append([round(X_test['X_0'][i],2),round(X_test['X_1'][i],2)])\n",
    "\n",
    "y_train = []\n",
    "\n",
    "for i in range(len(Y_train['y'])):\n",
    "    y_train.append([Y_train['y'][i]])\n",
    "    \n",
    "y_test = []\n",
    "\n",
    "for i in range(len(Y_test['y'])):\n",
    "    y_test.append([Y_test['y'][i]])\n",
    "'''\n",
    "\n",
    "training_data = np.empty((0,2), int)\n",
    "for i in range(len(X_train['X_0'])):\n",
    "    training_data = np.append(training_data, np.array([[X_train['X_0'][i],X_train['X_1'][i]]]), axis=0)\n",
    "    \n",
    "testing_data = np.empty((0,2), int)\n",
    "for i in range(len(X_test['X_0'])):\n",
    "    testing_data = np.append(testing_data, np.array([[X_test['X_0'][i],X_test['X_1'][i]]]), axis=0)\n",
    "\n",
    "\n",
    "\n",
    "y_train = np.empty((0,1), int)\n",
    "for i in range(len(Y_train['y'])):\n",
    "    y_train = np.append(y_train, np.array([[Y_train['y'][i]]]), axis=0)\n",
    "\n",
    "y_test = np.empty((0,1), int)\n",
    "for i in range(len(Y_test['y'])):\n",
    "    y_test = np.append(y_test, np.array([[Y_test['y'][i]]]), axis=0)\n",
    "\n",
    "np_testing_y = np.array(y_test)\n",
    "np_testing_x = np.array(testing_data)\n",
    "np_training_x = np.array(training_data)\n",
    "np_training_y = np.array(y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy-iteration: 54.88%\n",
      "Testing accuracy-iteration: 69.51% Learning rate --> 0.01\n",
      "Confusion matrix ---> TrueTrue: 39 TrueFalse: 18 FalseTrue: 23 FalseFalse 2\n",
      "Recall= 0.95\n",
      "Precision= 0.63\n"
     ]
    }
   ],
   "source": [
    "class Network_layer:\n",
    "    \n",
    "    def __init__(self, number_input, number_neurons, use_standard_activation):\n",
    "        #self.size = size\n",
    "        self.weights = (np.random.rand(number_input,number_neurons)*2) - 1\n",
    "        #self.weights = np.random.rand(number_input,number_neurons)\n",
    "        self.bias = np.zeros(number_neurons)\n",
    "        self.last_activation = None\n",
    "        self.use_standard_activation = use_standard_activation\n",
    "        self.error = None\n",
    "        self.delta = None\n",
    "        \n",
    "    def activate(self, x):\n",
    "        r = np.dot(x, self.weights) + self.bias\n",
    "        if self.use_standard_activation:\n",
    "            self.last_activation = self.activation(r)\n",
    "        else: \n",
    "            self.last_activation = self.tanh(r)\n",
    "            \n",
    "        return self.last_activation\n",
    "\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    def tanh(self, x):\n",
    "        return np.tanh(x)\n",
    "    \n",
    "    def activation(self, x):\n",
    "        #relu\n",
    "        if self.use_standard_activation:\n",
    "            x[x<0] = 0\n",
    "            return x\n",
    "        #tanh\n",
    "        return self.tanh(x)\n",
    "    \n",
    "    def activation_derivative(self, x):\n",
    "        #relu derivative\n",
    "        if self.use_standard_activation:\n",
    "            x[x>0] = 1\n",
    "            x[x<0] = 0\n",
    "            return x\n",
    "        #tanh derivative\n",
    "        return 1.0 - np.tanh(x)**2\n",
    "    \n",
    "\n",
    "class Neaural_net:\n",
    "    layers = []\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "    \n",
    "    def add_layer(self, layer):\n",
    "        self.layers.append(layer)\n",
    "    \n",
    "    def feed_forward(self, X):\n",
    "        for layer in self.layers:\n",
    "            X = layer.activate(X)\n",
    "        return X\n",
    "\n",
    "    def softmax(self,X):\n",
    "        exps = np.exp(X)\n",
    "        return exps / np.sum(exps)\n",
    "    \n",
    "    def cross_entropy(self, X,y):\n",
    "        m = y.shape[0]\n",
    "        p = self.softmax(X)\n",
    "        \n",
    "        likelihood = -np.log(p[range(m), y.argmax(axis=1)])\n",
    "        loss = np.sum(likelihood) / m\n",
    "        return loss\n",
    "        \n",
    "    def logloss(self, true_label, predicted, eps=1e-15):\n",
    "        p = np.clip(predicted, eps, 1 - eps)\n",
    "        if true_label == 1:\n",
    "            return -math.log(p)\n",
    "        else:\n",
    "            return -math.log(1 - p) \n",
    "        \n",
    "    def mae(self, targets, predictions):\n",
    "        differences = predictions - targets\n",
    "        absolute_differences = np.absolute(differences)\n",
    "        mean_absolute_differences = absolute_differences.mean()\n",
    "        return mean_absolute_differences\n",
    "        \n",
    "    def mean_squared_error(self, actual, predicted):\n",
    "        sum_square_error = 0.0\n",
    "        for i in range(len(actual)):\n",
    "            sum_square_error += (actual[i] - predicted[i])**2.0\n",
    "        mean_square_error = sum_square_error / len(actual)\n",
    "\n",
    "        return mean_square_error\n",
    "        \n",
    "    def checkPrediction(self, y, pred):\n",
    "        pred = np.around(pred)\n",
    "        if pred == y:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "               \n",
    "        \n",
    "    def backpropagation(self, X, y, learning_rate):\n",
    "        output = self.feed_forward(X)\n",
    "        average_activation = None\n",
    "        correct_predictions = 0\n",
    "        for i in reversed(range(len(self.layers))):\n",
    "            layer = self.layers[i]\n",
    "            all_deltas = []\n",
    "            last_activations = []\n",
    "            for j in range(len(output)):\n",
    "                if layer == self.layers[-1]:\n",
    "                    layer.error =  y[j] - output[j]\n",
    "                    all_deltas.append(layer.error * layer.activation_derivative(output[j]))\n",
    "                    last_activations.append(layer.activation_derivative(output[j]))\n",
    "                    correct_predictions = correct_predictions + self.checkPrediction(y[j], output[j])\n",
    "                else:\n",
    "                    next_layer = self.layers[i + 1]\n",
    "                    layer.error = np.dot(next_layer.weights, next_layer.delta)\n",
    "                    all_deltas.append(layer.error * layer.activation_derivative(layer.last_activation[j]))\n",
    "                    last_activations.append(layer.activation_derivative(layer.last_activation[j]))\n",
    "                    \n",
    "            average_delta = sum(all_deltas)/float(len(output))\n",
    "            average_activation = sum(last_activations)/float(len(output))\n",
    "            layer.delta = average_delta\n",
    "    \n",
    "        # Update the weights\n",
    "        for j in range(len(output)):\n",
    "            for i in range(len(self.layers)):\n",
    "                layer = self.layers[i]\n",
    "                input_to_use = np.atleast_2d(X[j] if i == 0 else self.layers[i - 1].last_activation[j])\n",
    "                layer.weights += layer.delta * input_to_use.T * learning_rate\n",
    "        return correct_predictions\n",
    "            \n",
    "    \n",
    "    \n",
    "    def get_batch(self, inputs, targets, batchsize, shuffle=False):\n",
    "        assert len(inputs) == len(targets)\n",
    "        if shuffle:\n",
    "            indices = np.random.permutation(len(inputs))\n",
    "        for start in range(0, len(inputs) - batchsize + 1, batchsize):\n",
    "            if shuffle:\n",
    "                excerpt = indices[start:start + batchsize]\n",
    "            else:\n",
    "                excerpt = slice(start, start + batchsize)\n",
    "            yield inputs[excerpt], targets[excerpt] \n",
    "\n",
    "            \n",
    "    def train(self, X, y, learning_rate, max_epochs, batchsize):\n",
    "        mses = []\n",
    "        training_accuracy = []\n",
    "        for i in range(max_epochs):\n",
    "            correct_per_epoch = 0\n",
    "            for x_batch,y_batch in self.get_batch(X,y,batchsize=batchsize,shuffle=False):\n",
    "                correct_predictions_batch = self.backpropagation(x_batch,y_batch, learning_rate)\n",
    "                correct_per_epoch = correct_per_epoch + correct_predictions_batch\n",
    "\n",
    "            if i % 100 == 0:\n",
    "                mse = np.mean(np.square(y - NN.feed_forward(X)))\n",
    "                mses.append(mse*100)\n",
    "                training_accuracy.append((correct_per_epoch/len(X))*100)\n",
    "        return mses, training_accuracy \n",
    "    \n",
    "    \n",
    "    \n",
    "    def test(self, X, y_true):\n",
    "        correct = 0\n",
    "        TT = 0 \n",
    "        TF = 0\n",
    "        FT = 0\n",
    "        FF = 0\n",
    "        for i in range(len(X)):\n",
    "            forward_pass = self.feed_forward(X[i])\n",
    "            pred = np.around(forward_pass)\n",
    "            y = y_true[i]\n",
    "            if pred == y:\n",
    "                correct = correct + 1\n",
    "            if pred == 1 and y == 1:\n",
    "                TT = TT + 1\n",
    "            if pred == 1 and y == 0:\n",
    "                FT = FT + 1\n",
    "            if pred == 0 and y == 1:\n",
    "                FF = FF + 1\n",
    "            if pred == 0 and y == 0:\n",
    "                TF = TF + 1\n",
    "            confusion = [TT, TF, FT, FF]\n",
    "        return correct/len(X), confusion\n",
    "\n",
    "\n",
    "\n",
    "NN = Neaural_net()\n",
    "NN.add_layer(Network_layer(2, 10, False))\n",
    "NN.add_layer(Network_layer(10, 10, False))\n",
    "NN.add_layer(Network_layer(10, 10, False))\n",
    "NN.add_layer(Network_layer(10,1, False))\n",
    "\n",
    "\n",
    "# Train the neural networks\n",
    "accuracy = []\n",
    "iteration = []\n",
    "learning_rate = 0.01\n",
    "total_error = []\n",
    "total_training_accuracy = []\n",
    "number_test = 25\n",
    "number_epoc = 40\n",
    "batchsize = 5\n",
    "last_mse = [0,0,0,0,0]\n",
    "a = 0\n",
    "total_time = 0\n",
    "max_accuracy = 0\n",
    "best_NN = copy.copy(NN)\n",
    "best_confusion = [0,0,0,0]\n",
    "best_learning_rate = learning_rate\n",
    "recall = []\n",
    "precision = []\n",
    "\n",
    "for i in range(1,number_test+1):\n",
    "    start = time.time()\n",
    "    errors, training_accuracy = NN.train(np_training_x, np_training_y, learning_rate, number_epoc, batchsize)\n",
    "    #print(\"first\", training_accuracy)\n",
    "    end = time.time()\n",
    "    total_time = total_time + (end-start)\n",
    "    total_error.append(errors)\n",
    "    total_training_accuracy.append(training_accuracy)\n",
    "    last_mse[a] = round(errors[0],2)\n",
    "    #print(last_mse[a])\n",
    "    a = a + 1\n",
    "    if a > 4:\n",
    "        a = 0\n",
    "    iteration.append(i*number_epoc)\n",
    "    accuracy_one, confusion = NN.test(np_testing_x, np_testing_y.flatten())\n",
    "    if(accuracy_one > max_accuracy):\n",
    "        max_accuracy = accuracy_one\n",
    "        best_NN = copy.deepcopy(NN)\n",
    "        best_confusion = confusion\n",
    "        best_learning_rate = learning_rate\n",
    "    print('Training accuracy-iteration: %.2f%%' % (training_accuracy[-1]))\n",
    "    print('Testing accuracy-iteration: %.2f%%' % (accuracy_one*100), \"Learning rate --> \"+ str(learning_rate))\n",
    "    print('Confusion matrix ---> TrueTrue:',confusion[0] ,'TrueFalse:',confusion[1], 'FalseTrue:',confusion[2] , 'FalseFalse',confusion[3] )\n",
    "    recall_tmp = round(confusion[0]/(confusion[0]+confusion[3]), 2)\n",
    "    precision_tmp = round(confusion[0]/(confusion[0]+confusion[2]), 2)\n",
    "    recall.append(recall_tmp)\n",
    "    print('Recall=', recall_tmp)\n",
    "    precision.append(precision_tmp)\n",
    "    print('Precision=', precision_tmp)\n",
    "    accuracy.append(accuracy_one*100)\n",
    "    mse_same = True\n",
    "    for i in range(len(last_mse)):\n",
    "        if round(last_mse[i],2) != round(errors[0],2):\n",
    "            mse_same = False\n",
    "    if mse_same:\n",
    "        print(\"MSE has converged!\")\n",
    "        break;\n",
    "    learning_rate = learning_rate*0.99\n",
    "\n",
    "\n",
    "\n",
    "print(\"Average training time for network running \" + str(number_epoc) + \" iterations with batch size \" + str(batchsize) + \" --->\" + str(total_time/number_test)+ \"s\")\n",
    "\n",
    "plt.plot(iteration,accuracy, label='Validation accuracy')\n",
    "plt.plot(iteration, total_error, label='Mean squared error')\n",
    "plt.plot(iteration, total_training_accuracy, label='Training accuracy')\n",
    "#plt.plot(iteration, recall, label='Recall')\n",
    "#plt.plot(iteration, precision, label='Precision')\n",
    "plt.xlabel('iterations')\n",
    "plt.ylabel('Accuracy & MSE(*100)')\n",
    "plt.axis((number_epoc,number_epoc*number_test,0,100))\n",
    "plt.title('Changes in Accuracy')\n",
    "plt.legend(loc='best')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "accuracy_one, confusion = best_NN.test(np_testing_x, np_testing_y.flatten())\n",
    "print(\"------------------------\")\n",
    "print('Accuracy-best: %.2f%%' % (accuracy_one*100), \"Learning rate --> \"+ str(best_learning_rate))\n",
    "print('TrueOne:',confusion[0] ,'TrueZero:',confusion[1], 'FalseOne:',confusion[2] , 'FalseZero',confusion[3] )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. (10 points) Activation and Loss functions. \n",
    "Please choose suitable activation functions φ(0), φ(1), φ(2) and a suitable Loss function to perform the task. Report and justify your choices in the report."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. (10 points) Learning rate, batch size, initialization. \n",
    "Please choose a suitable learning rate, batch size, initialization of the parameter values, and any other setting you may need. Discuss and justify your choices in the report."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. (10 points) Training. \n",
    "Make plots with the loss function computed over the training set and over the validation set. Stop the training when the error is small enough. Justify your stopping criterium. Report the final accuracy obtained and the confusion matrix on the validation dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. (20 points) Implementation. \n",
    "We will run and check the uploaded Python file. To obtain the points for this subproblem, the Python file has to run (no errors) and the MLP model and the Backpropagation algorithm have to be implemented completely from scratch by you. You are not allowed to use any library which implements MLP models, but you are allowed to use auxiliary libraries, e.g. Numpy, Matplotlib, Pandas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Peer Review paragraph (0 points)\n",
    "Finally, each group member must write a single paragraph outlining their opinion on the work distri- bution within the group. Did every group member contribute equally? Did you split up tasks in a fair manner, or jointly worked through the exercises. Do you think that some members of your group deserve a different grade from others?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
