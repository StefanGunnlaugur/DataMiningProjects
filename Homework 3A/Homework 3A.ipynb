{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Y'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2896\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2897\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2898\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Y'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-eeef3b3df432>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Y'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m     \u001b[0mtraining_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Y'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2978\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2979\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2980\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2981\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2982\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2897\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2898\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2899\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2900\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2901\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Y'"
     ]
    }
   ],
   "source": [
    "data_test = pd.read_excel(\"HW3Avalidate.xlsx\") \n",
    "data_train = pd.read_excel(\"HW3Atrain.xlsx\")\n",
    "\n",
    "X_train = data_train.copy()\n",
    "X_train = X_train.drop('y', axis=1)\n",
    "Y_train = data_train.copy()\n",
    "Y_train = Y_train.drop('X_0', axis=1)\n",
    "Y_train = Y_train.drop('X_1', axis=1)\n",
    "\n",
    "X_test = data_test.copy()\n",
    "X_test = X_test.drop('y', axis=1)\n",
    "Y_test = data_test.copy()\n",
    "Y_test = Y_test.drop('X_0', axis=1)\n",
    "Y_test = Y_test.drop('X_1', axis=1)\n",
    "\n",
    "\n",
    "x_0_min = min(X_train['X_0'])\n",
    "x_0_max = max(X_train['X_0'])\n",
    "\n",
    "x_1_min = min(X_train['X_1'])\n",
    "x_1_max = max(X_train['X_1'])\n",
    "\n",
    "test_x_0_min = min(X_test['X_0'])\n",
    "test_x_0_max = max(X_test['X_0'])\n",
    "\n",
    "test_x_1_min = min(X_test['X_1'])\n",
    "test_x_1_max = max(X_test['X_1'])\n",
    "\n",
    "for i in range(len(X_train['X_0'])):\n",
    "    X_train['X_0'][i] = (X_train['X_0'][i]-x_0_min)/(x_0_max-x_0_min) #formula used for Min-Max\n",
    "    X_train['X_1'][i] = (X_train['X_1'][i]-x_1_min)/(x_1_max-x_1_min) #formula used for Min-Max\n",
    "    \n",
    "for i in range(len(X_test['X_0'])):\n",
    "    X_test['X_0'][i] = (X_test['X_0'][i]-x_0_min)/(x_0_max-x_0_min) #formula used for Min-Max\n",
    "    X_test['X_1'][i] = (X_test['X_1'][i]-x_1_min)/(x_1_max-x_1_min) #formula used for Min-Max\n",
    "\n",
    "training_data = []\n",
    "for i in range(len(X_train['X_0'])):\n",
    "    training_data.append(np.array([X_train['X_0'][i],X_train['X_1'][i]]))\n",
    "    \n",
    "y_train = []\n",
    "for i in range(len(X_train['Y'])):\n",
    "    training_data.append(np.array([X_train['Y'][i]]))\n",
    "    \n",
    "testing_data = []\n",
    "for i in range(len(X_test['X_0'])):\n",
    "    testing_data.append(np.array([X_test['X_0'][i],X_test['X_1'][i]]))\n",
    "#print(testing_data)\n",
    "#print(Y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23.67\n",
      "Accuracy-iteration: 69.51% Learning rate --> 0.00999\n",
      "22.84\n",
      "Accuracy-iteration: 69.51% Learning rate --> 0.009980010000000001\n",
      "21.3\n",
      "Accuracy-iteration: 86.59% Learning rate --> 0.00997002999\n",
      "12.3\n",
      "Accuracy-iteration: 89.02% Learning rate --> 0.00996005996001\n",
      "10.59\n",
      "Accuracy-iteration: 87.80% Learning rate --> 0.00995009990004999\n",
      "9.51\n",
      "Accuracy-iteration: 85.37% Learning rate --> 0.00994014980014994\n",
      "8.84\n",
      "Accuracy-iteration: 85.37% Learning rate --> 0.00993020965034979\n",
      "8.5\n",
      "Accuracy-iteration: 85.37% Learning rate --> 0.009920279440699441\n",
      "8.29\n",
      "Accuracy-iteration: 85.37% Learning rate --> 0.009910359161258741\n",
      "8.15\n",
      "Accuracy-iteration: 85.37% Learning rate --> 0.009900448802097483\n",
      "8.06\n",
      "Accuracy-iteration: 85.37% Learning rate --> 0.009890548353295385\n",
      "8.0\n",
      "Accuracy-iteration: 85.37% Learning rate --> 0.00988065780494209\n",
      "7.95\n",
      "Accuracy-iteration: 85.37% Learning rate --> 0.009870777147137147\n",
      "7.91\n",
      "Accuracy-iteration: 85.37% Learning rate --> 0.00986090636999001\n",
      "7.87\n",
      "Accuracy-iteration: 85.37% Learning rate --> 0.00985104546362002\n",
      "7.83\n",
      "Accuracy-iteration: 85.37% Learning rate --> 0.0098411944181564\n",
      "7.79\n",
      "Accuracy-iteration: 85.37% Learning rate --> 0.009831353223738242\n",
      "7.75\n",
      "Accuracy-iteration: 84.15% Learning rate --> 0.009821521870514504\n",
      "7.69\n",
      "Accuracy-iteration: 84.15% Learning rate --> 0.00981170034864399\n",
      "7.62\n",
      "Accuracy-iteration: 85.37% Learning rate --> 0.009801888648295346\n",
      "7.53\n",
      "Accuracy-iteration: 85.37% Learning rate --> 0.00979208675964705\n",
      "7.42\n",
      "Accuracy-iteration: 87.80% Learning rate --> 0.009782294672887402\n",
      "7.28\n",
      "Accuracy-iteration: 89.02% Learning rate --> 0.009772512378214516\n",
      "7.13\n",
      "Accuracy-iteration: 89.02% Learning rate --> 0.009762739865836301\n",
      "6.97\n",
      "Accuracy-iteration: 90.24% Learning rate --> 0.009752977125970464\n",
      "6.8\n",
      "Accuracy-iteration: 90.24% Learning rate --> 0.009743224148844494\n",
      "6.63\n",
      "Accuracy-iteration: 90.24% Learning rate --> 0.009733480924695649\n",
      "6.46\n",
      "Accuracy-iteration: 91.46% Learning rate --> 0.009723747443770954\n",
      "6.29\n",
      "Accuracy-iteration: 91.46% Learning rate --> 0.009714023696327182\n",
      "6.12\n",
      "Accuracy-iteration: 91.46% Learning rate --> 0.009704309672630855\n",
      "5.96\n",
      "Accuracy-iteration: 92.68% Learning rate --> 0.009694605362958224\n",
      "5.8\n",
      "Accuracy-iteration: 92.68% Learning rate --> 0.009684910757595266\n",
      "5.64\n",
      "Accuracy-iteration: 92.68% Learning rate --> 0.00967522584683767\n",
      "5.5\n",
      "Accuracy-iteration: 92.68% Learning rate --> 0.009665550620990832\n",
      "5.37\n",
      "Accuracy-iteration: 92.68% Learning rate --> 0.009655885070369841\n",
      "5.26\n",
      "Accuracy-iteration: 92.68% Learning rate --> 0.009646229185299471\n",
      "5.17\n",
      "Accuracy-iteration: 92.68% Learning rate --> 0.009636582956114172\n",
      "5.1\n",
      "Accuracy-iteration: 92.68% Learning rate --> 0.009626946373158057\n",
      "5.05\n",
      "Accuracy-iteration: 92.68% Learning rate --> 0.0096173194267849\n",
      "5.01\n",
      "Accuracy-iteration: 92.68% Learning rate --> 0.009607702107358115\n",
      "4.97\n",
      "Accuracy-iteration: 92.68% Learning rate --> 0.009598094405250756\n",
      "4.94\n",
      "Accuracy-iteration: 92.68% Learning rate --> 0.009588496310845506\n",
      "4.92\n",
      "Accuracy-iteration: 92.68% Learning rate --> 0.00957890781453466\n",
      "4.89\n",
      "Accuracy-iteration: 92.68% Learning rate --> 0.009569328906720125\n",
      "4.87\n",
      "Accuracy-iteration: 93.90% Learning rate --> 0.009559759577813404\n",
      "4.85\n",
      "Accuracy-iteration: 93.90% Learning rate --> 0.009550199818235591\n",
      "4.84\n",
      "Accuracy-iteration: 93.90% Learning rate --> 0.009540649618417356\n",
      "4.82\n",
      "Accuracy-iteration: 93.90% Learning rate --> 0.009531108968798939\n",
      "4.81\n",
      "Accuracy-iteration: 93.90% Learning rate --> 0.00952157785983014\n",
      "4.79\n",
      "Accuracy-iteration: 93.90% Learning rate --> 0.00951205628197031\n",
      "4.78\n",
      "Accuracy-iteration: 93.90% Learning rate --> 0.00950254422568834\n",
      "4.77\n",
      "Accuracy-iteration: 93.90% Learning rate --> 0.009493041681462651\n",
      "4.76\n",
      "Accuracy-iteration: 93.90% Learning rate --> 0.009483548639781188\n",
      "4.74\n",
      "Accuracy-iteration: 93.90% Learning rate --> 0.009474065091141407\n",
      "4.73\n",
      "Accuracy-iteration: 93.90% Learning rate --> 0.009464591026050266\n",
      "4.72\n",
      "Accuracy-iteration: 93.90% Learning rate --> 0.009455126435024216\n",
      "4.71\n",
      "Accuracy-iteration: 93.90% Learning rate --> 0.009445671308589192\n",
      "4.7\n",
      "Accuracy-iteration: 93.90% Learning rate --> 0.009436225637280603\n",
      "4.69\n",
      "Accuracy-iteration: 93.90% Learning rate --> 0.009426789411643322\n",
      "4.68\n",
      "Accuracy-iteration: 93.90% Learning rate --> 0.009417362622231678\n",
      "4.68\n",
      "Accuracy-iteration: 93.90% Learning rate --> 0.009407945259609446\n",
      "4.67\n",
      "Accuracy-iteration: 93.90% Learning rate --> 0.009398537314349836\n",
      "4.66\n",
      "Accuracy-iteration: 93.90% Learning rate --> 0.009389138777035486\n",
      "4.65\n",
      "Accuracy-iteration: 93.90% Learning rate --> 0.009379749638258451\n",
      "4.64\n",
      "Accuracy-iteration: 93.90% Learning rate --> 0.009370369888620193\n",
      "4.63\n",
      "Accuracy-iteration: 93.90% Learning rate --> 0.009360999518731573\n",
      "4.62\n",
      "Accuracy-iteration: 93.90% Learning rate --> 0.009351638519212842\n",
      "4.62\n",
      "Accuracy-iteration: 93.90% Learning rate --> 0.009342286880693629\n",
      "4.61\n",
      "Accuracy-iteration: 93.90% Learning rate --> 0.009332944593812936\n",
      "4.6\n",
      "Accuracy-iteration: 93.90% Learning rate --> 0.009323611649219122\n",
      "4.59\n",
      "Accuracy-iteration: 93.90% Learning rate --> 0.009314288037569902\n",
      "4.58\n",
      "Accuracy-iteration: 93.90% Learning rate --> 0.009304973749532333\n",
      "4.58\n",
      "Accuracy-iteration: 95.12% Learning rate --> 0.0092956687757828\n",
      "4.57\n",
      "Accuracy-iteration: 95.12% Learning rate --> 0.009286373107007018\n",
      "4.56\n",
      "Accuracy-iteration: 95.12% Learning rate --> 0.00927708673390001\n",
      "4.55\n",
      "Accuracy-iteration: 95.12% Learning rate --> 0.00926780964716611\n",
      "4.55\n",
      "Accuracy-iteration: 95.12% Learning rate --> 0.009258541837518945\n",
      "4.54\n",
      "Accuracy-iteration: 95.12% Learning rate --> 0.009249283295681426\n",
      "4.53\n",
      "Accuracy-iteration: 95.12% Learning rate --> 0.009240034012385745\n",
      "4.52\n",
      "Accuracy-iteration: 95.12% Learning rate --> 0.009230793978373359\n",
      "4.51\n",
      "Accuracy-iteration: 95.12% Learning rate --> 0.009221563184394986\n",
      "4.51\n",
      "Accuracy-iteration: 95.12% Learning rate --> 0.00921234162121059\n",
      "4.5\n",
      "Accuracy-iteration: 95.12% Learning rate --> 0.00920312927958938\n",
      "4.49\n",
      "Accuracy-iteration: 95.12% Learning rate --> 0.00919392615030979\n",
      "4.49\n",
      "Accuracy-iteration: 95.12% Learning rate --> 0.00918473222415948\n",
      "4.48\n",
      "Accuracy-iteration: 95.12% Learning rate --> 0.009175547491935322\n",
      "4.47\n",
      "Accuracy-iteration: 95.12% Learning rate --> 0.009166371944443386\n",
      "4.47\n",
      "Accuracy-iteration: 95.12% Learning rate --> 0.009157205572498942\n",
      "4.46\n",
      "Accuracy-iteration: 95.12% Learning rate --> 0.009148048366926443\n",
      "4.46\n",
      "Accuracy-iteration: 95.12% Learning rate --> 0.009138900318559516\n",
      "4.45\n",
      "Accuracy-iteration: 95.12% Learning rate --> 0.009129761418240956\n",
      "4.45\n",
      "Accuracy-iteration: 95.12% Learning rate --> 0.009120631656822715\n",
      "4.44\n",
      "Accuracy-iteration: 95.12% Learning rate --> 0.009111511025165893\n",
      "4.44\n",
      "Accuracy-iteration: 95.12% Learning rate --> 0.009102399514140727\n",
      "4.43\n",
      "Accuracy-iteration: 95.12% Learning rate --> 0.009093297114626586\n",
      "4.43\n",
      "Accuracy-iteration: 95.12% Learning rate --> 0.00908420381751196\n",
      "4.43\n",
      "Accuracy-iteration: 95.12% Learning rate --> 0.009075119613694447\n",
      "4.42\n",
      "Accuracy-iteration: 95.12% Learning rate --> 0.009066044494080753\n",
      "4.42\n",
      "Accuracy-iteration: 95.12% Learning rate --> 0.009056978449586672\n",
      "4.42\n",
      "Accuracy-iteration: 95.12% Learning rate --> 0.009047921471137085\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "class Network_layer:\n",
    "    \n",
    "    def __init__(self, number_input, number_neurons, use_relu):\n",
    "        #self.size = size\n",
    "        self.weights = np.random.rand(number_input,number_neurons)\n",
    "        self.bias = np.zeros(number_neurons)\n",
    "        self.last_activation = None\n",
    "        self.use_relu = use_relu\n",
    "        self.error = None\n",
    "        self.delta = None\n",
    "        \n",
    "    def activate(self, x):\n",
    "        r = np.dot(x, self.weights) + self.bias\n",
    "        if self.use_relu:\n",
    "            self.last_activation = self.relu(r)\n",
    "        else: \n",
    "            self.last_activation = self.sigmoid(r)\n",
    "            \n",
    "        return self.last_activation\n",
    "    \n",
    "    def relu(self, x):\n",
    "        #x[x<0] = 0\n",
    "        \n",
    "        return np.tanh(x)\n",
    "                \n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    def activation_derivative(self, x):\n",
    "        if self.use_relu:\n",
    "            #x[x>0] = 1\n",
    "            #x[x<0] = 0\n",
    "            return 1.0 - np.tanh(x)**2\n",
    "        return x * (1 - x)\n",
    "    \n",
    "\n",
    "class Neaural_net:\n",
    "    layers = []\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "    \n",
    "    def add_layer(self, layer):\n",
    "        self.layers.append(layer)\n",
    "    \n",
    "    def feed_forward(self, X):\n",
    "        for layer in self.layers:\n",
    "            X = layer.activate(X)\n",
    "        return X\n",
    "\n",
    "        \n",
    "    def backpropagation(self, X, y, learning_rate):\n",
    "        output = self.feed_forward(X)\n",
    "        for i in reversed(range(len(self.layers))):\n",
    "            layer = self.layers[i]\n",
    "\n",
    "            if layer == self.layers[-1]:\n",
    "                layer.error = y - output\n",
    "                layer.delta = layer.error * layer.activation_derivative(output)\n",
    "            else:\n",
    "                next_layer = self.layers[i + 1]\n",
    "                layer.error = np.dot(next_layer.weights, next_layer.delta)\n",
    "                layer.delta = layer.error * layer.activation_derivative(layer.last_activation)\n",
    "\n",
    "        # Update the weights\n",
    "        for i in range(len(self.layers)):\n",
    "            layer = self.layers[i]\n",
    "            input_to_use = np.atleast_2d(X if i == 0 else self.layers[i - 1].last_activation)\n",
    "            layer.weights += layer.delta * input_to_use.T * learning_rate\n",
    "            \n",
    "    def train(self, X, y, learning_rate, max_epochs):\n",
    "        mses = []\n",
    "\n",
    "        for i in range(max_epochs):\n",
    "            for j in range(len(X)):\n",
    "                self.backpropagation(X[j], y[j], learning_rate)\n",
    "            if i % 100 == 0:\n",
    "                mse = np.mean(np.square(y - NN.feed_forward(X)))\n",
    "                mses.append(mse*100)\n",
    "                #print('Epoch: #%s, MSE: %f' % (i, float(mse)))\n",
    "\n",
    "        return mses\n",
    "    \n",
    "    def test(self, X, y_true):\n",
    "        correct = 0\n",
    "        \n",
    "        for i in range(len(X)):\n",
    "            forward_pass = self.feed_forward(X[i])\n",
    "            if np.around(forward_pass) == y_true[i]:\n",
    "                correct = correct + 1\n",
    "        #y_pred = 0\n",
    "        \n",
    "        #if forward_pass.ndim == 1:\n",
    "        #    y_pred = np.argmax(forward_pass)\n",
    "        #else:\n",
    "        #    y_pred = np.argmax(forward_pass, axis=1)\n",
    "\n",
    "#        return (y_pred == y_true).mean()\n",
    "        return correct/len(X)\n",
    "\n",
    "data_test = pd.read_excel(\"HW3Avalidate.xlsx\") \n",
    "data_train = pd.read_excel(\"HW3Atrain.xlsx\")\n",
    "\n",
    "X_train = data_train.copy()\n",
    "X_train = X_train.drop('y', axis=1)\n",
    "Y_train = data_train.copy()\n",
    "Y_train = Y_train.drop('X_0', axis=1)\n",
    "Y_train = Y_train.drop('X_1', axis=1)\n",
    "\n",
    "X_test = data_test.copy()\n",
    "X_test = X_test.drop('y', axis=1)\n",
    "Y_test = data_test.copy()\n",
    "Y_test = Y_test.drop('X_0', axis=1)\n",
    "Y_test = Y_test.drop('X_1', axis=1)\n",
    "\n",
    "\n",
    "x_0_min = min(X_train['X_0'])\n",
    "x_0_max = max(X_train['X_0'])\n",
    "\n",
    "x_1_min = min(X_train['X_1'])\n",
    "x_1_max = max(X_train['X_1'])\n",
    "\n",
    "test_x_0_min = min(X_test['X_0'])\n",
    "test_x_0_max = max(X_test['X_0'])\n",
    "\n",
    "test_x_1_min = min(X_test['X_1'])\n",
    "test_x_1_max = max(X_test['X_1'])\n",
    "\n",
    "for i in range(len(X_train['X_0'])):\n",
    "    X_train['X_0'][i] = (X_train['X_0'][i]-x_0_min)/(x_0_max-x_0_min) #formula used for Min-Max\n",
    "    X_train['X_1'][i] = (X_train['X_1'][i]-x_1_min)/(x_1_max-x_1_min) #formula used for Min-Max\n",
    "    \n",
    "for i in range(len(X_test['X_0'])):\n",
    "    X_test['X_0'][i] = (X_test['X_0'][i]-x_0_min)/(x_0_max-x_0_min) #formula used for Min-Max\n",
    "    X_test['X_1'][i] = (X_test['X_1'][i]-x_1_min)/(x_1_max-x_1_min) #formula used for Min-Max\n",
    "\n",
    "training_data = []\n",
    "for i in range(len(X_train['X_0'])):\n",
    "    training_data.append([round(X_train['X_0'][i], 2),round(X_train['X_1'][i],2)])\n",
    "    \n",
    "testing_data = []\n",
    "for i in range(len(X_test['X_0'])):\n",
    "    testing_data.append([round(X_test['X_0'][i],2),round(X_test['X_1'][i],2)])\n",
    "\n",
    "y_train = []\n",
    "\n",
    "for i in range(len(Y_train['y'])):\n",
    "    y_train.append([Y_train['y'][i]])\n",
    "    \n",
    "y_test = []\n",
    "\n",
    "for i in range(len(Y_test['y'])):\n",
    "    y_test.append([Y_test['y'][i]])\n",
    "\n",
    "np_testing_y = np.array(y_test)\n",
    "np_testing_x = np.array(testing_data)\n",
    "np_training_x = np.array(training_data)\n",
    "np_training_y = np.array(y_train)\n",
    "\n",
    "#print(testing_data)\n",
    "NN = Neaural_net()\n",
    "NN.add_layer(Network_layer(2, 10, True))\n",
    "NN.add_layer(Network_layer(10, 10, True))\n",
    "NN.add_layer(Network_layer(10,1, True))\n",
    "\n",
    "\n",
    "# Train the neural networks\n",
    "accuracy = []\n",
    "iteration = []\n",
    "#print(NN.feed_forward(np_training_x[0]))\n",
    "learning_rate = 0.01\n",
    "total_error = []\n",
    "number_test = 100\n",
    "number_epoc = 20\n",
    "last_mse = [0,0,0,0,0]\n",
    "a = 0\n",
    "for i in range(1,number_test+1):\n",
    "    errors = NN.train(np_training_x, np_training_y, learning_rate, number_epoc)\n",
    "    total_error.append(errors)\n",
    "    learning_rate = learning_rate*0.999\n",
    "    last_mse[a] = round(errors[0],2)\n",
    "    print(last_mse[a])\n",
    "    a = a + 1\n",
    "    if a > 4:\n",
    "        a = 0\n",
    "    iteration.append(i*number_epoc)\n",
    "    accuracy_one = NN.test(np_testing_x, np_testing_y.flatten())\n",
    "    print('Accuracy-iteration: %.2f%%' % (accuracy_one*100), \"Learning rate --> \"+ str(learning_rate))\n",
    "    accuracy.append(accuracy_one*100)\n",
    "    mse_same = True\n",
    "    for i in range(len(last_mse)):\n",
    "        if round(last_mse[i],2) != round(errors[0],2):\n",
    "            mse_same = False\n",
    "    if mse_same:\n",
    "        print(\"MSE has converged!\")\n",
    "        break;\n",
    "\n",
    "\n",
    "plt.plot(iteration,accuracy)\n",
    "plt.plot(iteration, total_error)\n",
    "plt.xlabel('iterations')\n",
    "plt.ylabel('Accuracy & MSE(*100)')\n",
    "plt.axis((number_epoc,number_epoc*number_test,0,100))\n",
    "plt.title('Changes in Accuracy')\n",
    "plt.show()\n",
    "\n",
    "data = {'weights': []}\n",
    "data['weights'].append(NN.layers[0].weights.tolist())\n",
    "data['weights'].append(NN.layers[1].weights.tolist())\n",
    "data['weights'].append(NN.layers[2].weights.tolist())\n",
    "with open('data.txt', 'w') as outfile:\n",
    "    json.dump(data, outfile)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 10)\n",
      "[0.92255083 2.05405561 1.53219312 1.75838565 1.26657119 2.05530305\n",
      " 1.00730676 1.33028476 1.79629828 1.34743652]\n"
     ]
    }
   ],
   "source": [
    "a = np.array([1,2])\n",
    "b = np.array([1,2,3,4,5,6,7,8,9,10])\n",
    "b = np.random.rand(a.shape[0],10)\n",
    "print(b.shape)\n",
    "print(np.dot(a,b))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             X_0          X_1  y\n",
      "0    2917.211242  3289.522533  0\n",
      "1    1888.937716   781.528356  0\n",
      "2    4188.521414  1554.476261  0\n",
      "3    8145.555339  9804.066728  0\n",
      "4    9584.488981  6176.337189  0\n",
      "5    4039.633757   167.607143  0\n",
      "6    2399.020781  6251.891672  1\n",
      "7    5899.876835  7732.514778  0\n",
      "8    1832.517788  1300.730270  0\n",
      "9    1612.288975  6426.781763  1\n",
      "10   9286.626998  1562.956170  1\n",
      "11   8497.141947  8806.179833  0\n",
      "12   9355.215737  9845.547425  0\n",
      "13   6959.671139  7096.002717  1\n",
      "14   8318.774203  6561.487762  0\n",
      "15   2956.353881  1744.414739  0\n",
      "16   3494.320864  7175.690252  1\n",
      "17   9733.605268   222.444205  1\n",
      "18   3357.382654  9136.605932  1\n",
      "19   1466.794269  7637.730334  0\n",
      "20   2309.226297  2891.563126  0\n",
      "21   1640.887579  9801.869658  1\n",
      "22   5781.556560   248.504846  1\n",
      "23   3828.052410  2037.851361  0\n",
      "24    951.195475  2630.862744  0\n",
      "25   6948.834627  7701.470309  0\n",
      "26   7196.243632  9244.761921  0\n",
      "27    848.571206  1321.366007  0\n",
      "28    513.550491  9111.127856  1\n",
      "29   3146.851248  5865.060183  1\n",
      "..           ...          ... ..\n",
      "380  1354.479525   441.703601  0\n",
      "381  9260.183585  5774.402365  0\n",
      "382  8406.182725  8269.783190  0\n",
      "383  6503.097371  7482.153613  0\n",
      "384  7967.216126  5718.601756  0\n",
      "385  8429.821163  6399.605653  0\n",
      "386   820.737586  2881.869380  0\n",
      "387  3077.360989  8760.779859  1\n",
      "388  1059.786065  1635.171714  0\n",
      "389  1316.094041  1087.774188  0\n",
      "390  1897.924554  6710.567510  0\n",
      "391  3633.539851  9924.821213  1\n",
      "392  6207.019150  7788.362690  0\n",
      "393  4099.626829  8716.546133  1\n",
      "394   369.139194  9251.637157  1\n",
      "395  8819.261828  5893.924605  0\n",
      "396  1636.984145  7707.691392  1\n",
      "397  9591.814449  2207.322915  1\n",
      "398  5777.221351  8519.385712  0\n",
      "399  1606.933140  5911.313525  1\n",
      "400   799.139624  8920.322918  1\n",
      "401  3289.006632  6396.295332  1\n",
      "402  5924.272476  2173.446122  1\n",
      "403   525.864245  3760.108926  0\n",
      "404  8056.268167   321.017245  1\n",
      "405  1345.301027  7840.762858  1\n",
      "406  4162.846469  5930.753704  1\n",
      "407  1925.967024   590.183805  0\n",
      "408  1104.523856  4320.915987  0\n",
      "409  5638.326812  2839.513746  1\n",
      "\n",
      "[410 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "X_test = pd.read_excel(\"HW3Avalidate.xlsx\") \n",
    "X_train = pd.read_excel(\"HW3Atrain.xlsx\") \n",
    "\n",
    "'''\n",
    "h0 = np.dot(X, weights1) + bias1\n",
    "phi1 = relu(h0)\n",
    "\n",
    "h1 = np.dot(phi1, weights2) + bias2\n",
    "phi2 = relu(h1)\n",
    "\n",
    "h2 = np.dot(phi2, weights3) + bias3\n",
    "phi3 = relu(h2)\n",
    "\n",
    "h3 = np.dot(phi3, weights4) + bias4\n",
    "phi4 = sigmoid(h3)\n",
    "'''\n",
    "\n",
    "print(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. (10 points) Activation and Loss functions. \n",
    "Please choose suitable activation functions φ(0), φ(1), φ(2) and a suitable Loss function to perform the task. Report and justify your choices in the report."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. (10 points) Learning rate, batch size, initialization. \n",
    "Please choose a suitable learning rate, batch size, initialization of the parameter values, and any other setting you may need. Discuss and justify your choices in the report."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. (10 points) Training. \n",
    "Make plots with the loss function computed over the training set and over the validation set. Stop the training when the error is small enough. Justify your stopping criterium. Report the final accuracy obtained and the confusion matrix on the validation dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. (20 points) Implementation. \n",
    "We will run and check the uploaded Python file. To obtain the points for this subproblem, the Python file has to run (no errors) and the MLP model and the Backpropagation algorithm have to be implemented completely from scratch by you. You are not allowed to use any library which implements MLP models, but you are allowed to use auxiliary libraries, e.g. Numpy, Matplotlib, Pandas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Peer Review paragraph (0 points)\n",
    "Finally, each group member must write a single paragraph outlining their opinion on the work distri- bution within the group. Did every group member contribute equally? Did you split up tasks in a fair manner, or jointly worked through the exercises. Do you think that some members of your group deserve a different grade from others?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
