{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Y'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2896\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2897\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2898\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Y'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-eeef3b3df432>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Y'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m     \u001b[0mtraining_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Y'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2978\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2979\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2980\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2981\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2982\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2897\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2898\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2899\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2900\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2901\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Y'"
     ]
    }
   ],
   "source": [
    "data_test = pd.read_excel(\"HW3Avalidate.xlsx\") \n",
    "data_train = pd.read_excel(\"HW3Atrain.xlsx\")\n",
    "\n",
    "X_train = data_train.copy()\n",
    "X_train = X_train.drop('y', axis=1)\n",
    "Y_train = data_train.copy()\n",
    "Y_train = Y_train.drop('X_0', axis=1)\n",
    "Y_train = Y_train.drop('X_1', axis=1)\n",
    "\n",
    "X_test = data_test.copy()\n",
    "X_test = X_test.drop('y', axis=1)\n",
    "Y_test = data_test.copy()\n",
    "Y_test = Y_test.drop('X_0', axis=1)\n",
    "Y_test = Y_test.drop('X_1', axis=1)\n",
    "\n",
    "\n",
    "x_0_min = min(X_train['X_0'])\n",
    "x_0_max = max(X_train['X_0'])\n",
    "\n",
    "x_1_min = min(X_train['X_1'])\n",
    "x_1_max = max(X_train['X_1'])\n",
    "\n",
    "test_x_0_min = min(X_test['X_0'])\n",
    "test_x_0_max = max(X_test['X_0'])\n",
    "\n",
    "test_x_1_min = min(X_test['X_1'])\n",
    "test_x_1_max = max(X_test['X_1'])\n",
    "\n",
    "for i in range(len(X_train['X_0'])):\n",
    "    X_train['X_0'][i] = (X_train['X_0'][i]-x_0_min)/(x_0_max-x_0_min) #formula used for Min-Max\n",
    "    X_train['X_1'][i] = (X_train['X_1'][i]-x_1_min)/(x_1_max-x_1_min) #formula used for Min-Max\n",
    "    \n",
    "for i in range(len(X_test['X_0'])):\n",
    "    X_test['X_0'][i] = (X_test['X_0'][i]-x_0_min)/(x_0_max-x_0_min) #formula used for Min-Max\n",
    "    X_test['X_1'][i] = (X_test['X_1'][i]-x_1_min)/(x_1_max-x_1_min) #formula used for Min-Max\n",
    "\n",
    "training_data = []\n",
    "for i in range(len(X_train['X_0'])):\n",
    "    training_data.append(np.array([X_train['X_0'][i],X_train['X_1'][i]]))\n",
    "    \n",
    "y_train = []\n",
    "for i in range(len(X_train['Y'])):\n",
    "    training_data.append(np.array([X_train['Y'][i]]))\n",
    "    \n",
    "testing_data = []\n",
    "for i in range(len(X_test['X_0'])):\n",
    "    testing_data.append(np.array([X_test['X_0'][i],X_test['X_1'][i]]))\n",
    "#print(testing_data)\n",
    "#print(Y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23.64\n",
      "Accuracy-iteration: 68.29% Learning rate --> 0.00999\n",
      "19.86\n",
      "Accuracy-iteration: 90.24% Learning rate --> 0.009980010000000001\n",
      "7.73\n",
      "Accuracy-iteration: 91.46% Learning rate --> 0.00997002999\n",
      "6.99\n",
      "Accuracy-iteration: 91.46% Learning rate --> 0.00996005996001\n",
      "6.78\n",
      "Accuracy-iteration: 92.68% Learning rate --> 0.00995009990004999\n",
      "6.59\n",
      "Accuracy-iteration: 92.68% Learning rate --> 0.00994014980014994\n",
      "6.28\n",
      "Accuracy-iteration: 92.68% Learning rate --> 0.00993020965034979\n",
      "5.99\n",
      "Accuracy-iteration: 92.68% Learning rate --> 0.009920279440699441\n",
      "5.81\n",
      "Accuracy-iteration: 92.68% Learning rate --> 0.009910359161258741\n",
      "5.7\n",
      "Accuracy-iteration: 92.68% Learning rate --> 0.009900448802097483\n",
      "5.62\n",
      "Accuracy-iteration: 92.68% Learning rate --> 0.009890548353295385\n",
      "5.55\n",
      "Accuracy-iteration: 92.68% Learning rate --> 0.00988065780494209\n",
      "5.5\n",
      "Accuracy-iteration: 92.68% Learning rate --> 0.009870777147137147\n",
      "5.44\n",
      "Accuracy-iteration: 92.68% Learning rate --> 0.00986090636999001\n",
      "5.39\n",
      "Accuracy-iteration: 92.68% Learning rate --> 0.00985104546362002\n",
      "5.32\n",
      "Accuracy-iteration: 92.68% Learning rate --> 0.0098411944181564\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-3f46241cd3a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnumber_test\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m     \u001b[0merrors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp_training_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp_training_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_epoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m     \u001b[0mtotal_error\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[0mlearning_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m0.999\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-3f46241cd3a2>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, X, y, learning_rate, max_epochs)\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackpropagation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m                 \u001b[0mmse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msquare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mNN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-3f46241cd3a2>\u001b[0m in \u001b[0;36mbackpropagation\u001b[0;34m(self, X, y, learning_rate)\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m                 \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m                 \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation_derivative\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m                 \u001b[0mnext_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-3f46241cd3a2>\u001b[0m in \u001b[0;36mactivation_derivative\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0;31m#x[x>0] = 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0;31m#x[x<0] = 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;36m1.0\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "class Network_layer:\n",
    "    \n",
    "    def __init__(self, number_input, number_neurons, use_relu):\n",
    "        #self.size = size\n",
    "        self.weights = np.random.rand(number_input,number_neurons)\n",
    "        self.bias = np.zeros(number_neurons)\n",
    "        self.last_activation = None\n",
    "        self.use_relu = use_relu\n",
    "        self.error = None\n",
    "        self.delta = None\n",
    "        \n",
    "    def activate(self, x):\n",
    "        r = np.dot(x, self.weights) + self.bias\n",
    "        if self.use_relu:\n",
    "            self.last_activation = self.relu(r)\n",
    "        else: \n",
    "            self.last_activation = self.sigmoid(r)\n",
    "            \n",
    "        return self.last_activation\n",
    "    \n",
    "    def relu(self, x):\n",
    "        #x[x<0] = 0\n",
    "        \n",
    "        return np.tanh(x)\n",
    "                \n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    def activation_derivative(self, x):\n",
    "        if self.use_relu:\n",
    "            #x[x>0] = 1\n",
    "            #x[x<0] = 0\n",
    "            return 1.0 - np.tanh(x)**2\n",
    "        return x * (1 - x)\n",
    "    \n",
    "\n",
    "class Neaural_net:\n",
    "    layers = []\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "    \n",
    "    def add_layer(self, layer):\n",
    "        self.layers.append(layer)\n",
    "    \n",
    "    def feed_forward(self, X):\n",
    "        for layer in self.layers:\n",
    "            X = layer.activate(X)\n",
    "        return X\n",
    "\n",
    "        \n",
    "    def backpropagation(self, X, y, learning_rate):\n",
    "        output = self.feed_forward(X)\n",
    "        for i in reversed(range(len(self.layers))):\n",
    "            layer = self.layers[i]\n",
    "\n",
    "            if layer == self.layers[-1]:\n",
    "                layer.error = y - output\n",
    "                layer.delta = layer.error * layer.activation_derivative(output)\n",
    "            else:\n",
    "                next_layer = self.layers[i + 1]\n",
    "                layer.error = np.dot(next_layer.weights, next_layer.delta)\n",
    "                layer.delta = layer.error * layer.activation_derivative(layer.last_activation)\n",
    "\n",
    "        # Update the weights\n",
    "        for i in range(len(self.layers)):\n",
    "            layer = self.layers[i]\n",
    "            input_to_use = np.atleast_2d(X if i == 0 else self.layers[i - 1].last_activation)\n",
    "            layer.weights += layer.delta * input_to_use.T * learning_rate\n",
    "            \n",
    "    def train(self, X, y, learning_rate, max_epochs):\n",
    "        mses = []\n",
    "\n",
    "        for i in range(max_epochs):\n",
    "            for j in range(len(X)):\n",
    "                self.backpropagation(X[j], y[j], learning_rate)\n",
    "            if i % 100 == 0:\n",
    "                mse = np.mean(np.square(y - NN.feed_forward(X)))\n",
    "                mses.append(mse*100)\n",
    "                #print('Epoch: #%s, MSE: %f' % (i, float(mse)))\n",
    "\n",
    "        return mses\n",
    "    \n",
    "    def test(self, X, y_true):\n",
    "        correct = 0\n",
    "        \n",
    "        for i in range(len(X)):\n",
    "            forward_pass = self.feed_forward(X[i])\n",
    "            if np.around(forward_pass) == y_true[i]:\n",
    "                correct = correct + 1\n",
    "        #y_pred = 0\n",
    "        \n",
    "        #if forward_pass.ndim == 1:\n",
    "        #    y_pred = np.argmax(forward_pass)\n",
    "        #else:\n",
    "        #    y_pred = np.argmax(forward_pass, axis=1)\n",
    "\n",
    "#        return (y_pred == y_true).mean()\n",
    "        return correct/len(X)\n",
    "\n",
    "data_test = pd.read_excel(\"HW3Avalidate.xlsx\") \n",
    "data_train = pd.read_excel(\"HW3Atrain.xlsx\")\n",
    "\n",
    "X_train = data_train.copy()\n",
    "X_train = X_train.drop('y', axis=1)\n",
    "Y_train = data_train.copy()\n",
    "Y_train = Y_train.drop('X_0', axis=1)\n",
    "Y_train = Y_train.drop('X_1', axis=1)\n",
    "\n",
    "X_test = data_test.copy()\n",
    "X_test = X_test.drop('y', axis=1)\n",
    "Y_test = data_test.copy()\n",
    "Y_test = Y_test.drop('X_0', axis=1)\n",
    "Y_test = Y_test.drop('X_1', axis=1)\n",
    "\n",
    "\n",
    "x_0_min = min(X_train['X_0'])\n",
    "x_0_max = max(X_train['X_0'])\n",
    "\n",
    "x_1_min = min(X_train['X_1'])\n",
    "x_1_max = max(X_train['X_1'])\n",
    "\n",
    "test_x_0_min = min(X_test['X_0'])\n",
    "test_x_0_max = max(X_test['X_0'])\n",
    "\n",
    "test_x_1_min = min(X_test['X_1'])\n",
    "test_x_1_max = max(X_test['X_1'])\n",
    "\n",
    "for i in range(len(X_train['X_0'])):\n",
    "    X_train['X_0'][i] = (X_train['X_0'][i]-x_0_min)/(x_0_max-x_0_min) #formula used for Min-Max\n",
    "    X_train['X_1'][i] = (X_train['X_1'][i]-x_1_min)/(x_1_max-x_1_min) #formula used for Min-Max\n",
    "    \n",
    "for i in range(len(X_test['X_0'])):\n",
    "    X_test['X_0'][i] = (X_test['X_0'][i]-x_0_min)/(x_0_max-x_0_min) #formula used for Min-Max\n",
    "    X_test['X_1'][i] = (X_test['X_1'][i]-x_1_min)/(x_1_max-x_1_min) #formula used for Min-Max\n",
    "\n",
    "training_data = []\n",
    "for i in range(len(X_train['X_0'])):\n",
    "    training_data.append([round(X_train['X_0'][i], 2),round(X_train['X_1'][i],2)])\n",
    "    \n",
    "testing_data = []\n",
    "for i in range(len(X_test['X_0'])):\n",
    "    testing_data.append([round(X_test['X_0'][i],2),round(X_test['X_1'][i],2)])\n",
    "\n",
    "y_train = []\n",
    "\n",
    "for i in range(len(Y_train['y'])):\n",
    "    y_train.append([Y_train['y'][i]])\n",
    "    \n",
    "y_test = []\n",
    "\n",
    "for i in range(len(Y_test['y'])):\n",
    "    y_test.append([Y_test['y'][i]])\n",
    "\n",
    "np_testing_y = np.array(y_test)\n",
    "np_testing_x = np.array(testing_data)\n",
    "np_training_x = np.array(training_data)\n",
    "np_training_y = np.array(y_train)\n",
    "\n",
    "#print(testing_data)\n",
    "NN = Neaural_net()\n",
    "NN.add_layer(Network_layer(2, 10, True))\n",
    "NN.add_layer(Network_layer(10, 10, True))\n",
    "NN.add_layer(Network_layer(10,1, True))\n",
    "\n",
    "\n",
    "# Train the neural networks\n",
    "accuracy = []\n",
    "iteration = []\n",
    "#print(NN.feed_forward(np_training_x[0]))\n",
    "learning_rate = 0.01\n",
    "total_error = []\n",
    "number_test = 100\n",
    "number_epoc = 20\n",
    "last_mse = [0,0,0,0,0]\n",
    "a = 0\n",
    "for i in range(1,number_test+1):\n",
    "    errors = NN.train(np_training_x, np_training_y, learning_rate, number_epoc)\n",
    "    total_error.append(errors)\n",
    "    learning_rate = learning_rate*0.999\n",
    "    last_mse[a] = round(errors[0],2)\n",
    "    print(last_mse[a])\n",
    "    a = a + 1\n",
    "    if a > 4:\n",
    "        a = 0\n",
    "    iteration.append(i*number_epoc)\n",
    "    accuracy_one = NN.test(np_testing_x, np_testing_y.flatten())\n",
    "    print('Accuracy-iteration: %.2f%%' % (accuracy_one*100), \"Learning rate --> \"+ str(learning_rate))\n",
    "    accuracy.append(accuracy_one*100)\n",
    "    mse_same = True\n",
    "    for i in range(len(last_mse)):\n",
    "        if round(last_mse[i],2) != round(errors[0],2):\n",
    "            mse_same = False\n",
    "    if mse_same:\n",
    "        print(\"MSE has converged!\")\n",
    "        break;\n",
    "\n",
    "\n",
    "plt.plot(iteration,accuracy)\n",
    "plt.plot(iteration, total_error)\n",
    "plt.xlabel('iterations')\n",
    "plt.ylabel('Accuracy & MSE(*100)')\n",
    "plt.axis((number_epoc,number_epoc*number_test,0,100))\n",
    "plt.title('Changes in Accuracy')\n",
    "plt.show()\n",
    "\n",
    "data = {'weights': []}\n",
    "data['weights'].append(NN.layers[0].weights.tolist())\n",
    "data['weights'].append(NN.layers[1].weights.tolist())\n",
    "data['weights'].append(NN.layers[2].weights.tolist())\n",
    "with open('data.txt', 'w') as outfile:\n",
    "    json.dump(data, outfile)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 10)\n",
      "[0.92255083 2.05405561 1.53219312 1.75838565 1.26657119 2.05530305\n",
      " 1.00730676 1.33028476 1.79629828 1.34743652]\n"
     ]
    }
   ],
   "source": [
    "a = np.array([1,2])\n",
    "b = np.array([1,2,3,4,5,6,7,8,9,10])\n",
    "b = np.random.rand(a.shape[0],10)\n",
    "print(b.shape)\n",
    "print(np.dot(a,b))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             X_0          X_1  y\n",
      "0    2917.211242  3289.522533  0\n",
      "1    1888.937716   781.528356  0\n",
      "2    4188.521414  1554.476261  0\n",
      "3    8145.555339  9804.066728  0\n",
      "4    9584.488981  6176.337189  0\n",
      "5    4039.633757   167.607143  0\n",
      "6    2399.020781  6251.891672  1\n",
      "7    5899.876835  7732.514778  0\n",
      "8    1832.517788  1300.730270  0\n",
      "9    1612.288975  6426.781763  1\n",
      "10   9286.626998  1562.956170  1\n",
      "11   8497.141947  8806.179833  0\n",
      "12   9355.215737  9845.547425  0\n",
      "13   6959.671139  7096.002717  1\n",
      "14   8318.774203  6561.487762  0\n",
      "15   2956.353881  1744.414739  0\n",
      "16   3494.320864  7175.690252  1\n",
      "17   9733.605268   222.444205  1\n",
      "18   3357.382654  9136.605932  1\n",
      "19   1466.794269  7637.730334  0\n",
      "20   2309.226297  2891.563126  0\n",
      "21   1640.887579  9801.869658  1\n",
      "22   5781.556560   248.504846  1\n",
      "23   3828.052410  2037.851361  0\n",
      "24    951.195475  2630.862744  0\n",
      "25   6948.834627  7701.470309  0\n",
      "26   7196.243632  9244.761921  0\n",
      "27    848.571206  1321.366007  0\n",
      "28    513.550491  9111.127856  1\n",
      "29   3146.851248  5865.060183  1\n",
      "..           ...          ... ..\n",
      "380  1354.479525   441.703601  0\n",
      "381  9260.183585  5774.402365  0\n",
      "382  8406.182725  8269.783190  0\n",
      "383  6503.097371  7482.153613  0\n",
      "384  7967.216126  5718.601756  0\n",
      "385  8429.821163  6399.605653  0\n",
      "386   820.737586  2881.869380  0\n",
      "387  3077.360989  8760.779859  1\n",
      "388  1059.786065  1635.171714  0\n",
      "389  1316.094041  1087.774188  0\n",
      "390  1897.924554  6710.567510  0\n",
      "391  3633.539851  9924.821213  1\n",
      "392  6207.019150  7788.362690  0\n",
      "393  4099.626829  8716.546133  1\n",
      "394   369.139194  9251.637157  1\n",
      "395  8819.261828  5893.924605  0\n",
      "396  1636.984145  7707.691392  1\n",
      "397  9591.814449  2207.322915  1\n",
      "398  5777.221351  8519.385712  0\n",
      "399  1606.933140  5911.313525  1\n",
      "400   799.139624  8920.322918  1\n",
      "401  3289.006632  6396.295332  1\n",
      "402  5924.272476  2173.446122  1\n",
      "403   525.864245  3760.108926  0\n",
      "404  8056.268167   321.017245  1\n",
      "405  1345.301027  7840.762858  1\n",
      "406  4162.846469  5930.753704  1\n",
      "407  1925.967024   590.183805  0\n",
      "408  1104.523856  4320.915987  0\n",
      "409  5638.326812  2839.513746  1\n",
      "\n",
      "[410 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "X_test = pd.read_excel(\"HW3Avalidate.xlsx\") \n",
    "X_train = pd.read_excel(\"HW3Atrain.xlsx\") \n",
    "\n",
    "'''\n",
    "h0 = np.dot(X, weights1) + bias1\n",
    "phi1 = relu(h0)\n",
    "\n",
    "h1 = np.dot(phi1, weights2) + bias2\n",
    "phi2 = relu(h1)\n",
    "\n",
    "h2 = np.dot(phi2, weights3) + bias3\n",
    "phi3 = relu(h2)\n",
    "\n",
    "h3 = np.dot(phi3, weights4) + bias4\n",
    "phi4 = sigmoid(h3)\n",
    "'''\n",
    "\n",
    "print(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. (10 points) Activation and Loss functions. \n",
    "Please choose suitable activation functions φ(0), φ(1), φ(2) and a suitable Loss function to perform the task. Report and justify your choices in the report."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. (10 points) Learning rate, batch size, initialization. \n",
    "Please choose a suitable learning rate, batch size, initialization of the parameter values, and any other setting you may need. Discuss and justify your choices in the report."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. (10 points) Training. \n",
    "Make plots with the loss function computed over the training set and over the validation set. Stop the training when the error is small enough. Justify your stopping criterium. Report the final accuracy obtained and the confusion matrix on the validation dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. (20 points) Implementation. \n",
    "We will run and check the uploaded Python file. To obtain the points for this subproblem, the Python file has to run (no errors) and the MLP model and the Backpropagation algorithm have to be implemented completely from scratch by you. You are not allowed to use any library which implements MLP models, but you are allowed to use auxiliary libraries, e.g. Numpy, Matplotlib, Pandas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Peer Review paragraph (0 points)\n",
    "Finally, each group member must write a single paragraph outlining their opinion on the work distri- bution within the group. Did every group member contribute equally? Did you split up tasks in a fair manner, or jointly worked through the exercises. Do you think that some members of your group deserve a different grade from others?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
